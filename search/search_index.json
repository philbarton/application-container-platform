{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Application Container Platform # This page will give you an overview of ACP, the services it entails and the ways to get access to them. This is a general overview. Developers should check out the Developer Docs . The Application Container Platform provides an accredited, secure, scalable, reliable and performant hosting platform for Home Office applications.\u200b We use open source technology to allow delivery teams to develop, build, deploy and manage applications in a trusted environment. \u200b We focus on enabling self-service where possible, and have developed a Platform Hub to act as a one stop shop for all things ACP, including support requests for things that aren't self serve (yet). Benefits of using ACP # Reliance on specialist DevOps resource significantly reduced\u200b Central place to host applications\u200b Saves time for the projects\u200b Significant hosting cost reduction\u200b Common tools and patterns across projects i.e. CI\u200b Everyone using our platform MUST adhere to technical service standards . All Services will be under a review process. For definitions on each phase of the lifecycle and the criteria for production-ready, please refer here . Release notes for ACP services can be found here . Accessing the Services # When trying to get onto the platform for the first time you will need to get access to a various things that will allow you to get to other things. The full flow for this can be found here Support # You can find more information about support here If you require support you can talk to us on our Slack Channel or raise an issue on our Support Portal .","title":"Home"},{"location":"index.html#application-container-platform","text":"This page will give you an overview of ACP, the services it entails and the ways to get access to them. This is a general overview. Developers should check out the Developer Docs . The Application Container Platform provides an accredited, secure, scalable, reliable and performant hosting platform for Home Office applications.\u200b We use open source technology to allow delivery teams to develop, build, deploy and manage applications in a trusted environment. \u200b We focus on enabling self-service where possible, and have developed a Platform Hub to act as a one stop shop for all things ACP, including support requests for things that aren't self serve (yet).","title":"Application Container Platform"},{"location":"index.html#benefits-of-using-acp","text":"Reliance on specialist DevOps resource significantly reduced\u200b Central place to host applications\u200b Saves time for the projects\u200b Significant hosting cost reduction\u200b Common tools and patterns across projects i.e. CI\u200b Everyone using our platform MUST adhere to technical service standards . All Services will be under a review process. For definitions on each phase of the lifecycle and the criteria for production-ready, please refer here . Release notes for ACP services can be found here .","title":"Benefits of using ACP"},{"location":"index.html#accessing-the-services","text":"When trying to get onto the platform for the first time you will need to get access to a various things that will allow you to get to other things. The full flow for this can be found here","title":"Accessing the Services"},{"location":"index.html#support","text":"You can find more information about support here If you require support you can talk to us on our Slack Channel or raise an issue on our Support Portal .","title":"Support"},{"location":"external-addresses.html","text":"ACP External Addresses # The ACP clusters all run behind NAT gateways, with fixed external addresses. Note, unless you've specifically chosen to run in a specific availability zone, your pods can run in any of the AZs for that cluster, so all three addresses should be taken as the external IPs. Cluster Zone External Address ACP-CI eu-west-2a 52.56.254.215 eu-west-2b 35.176.205.192 eu-west-2c 35.178.14.117 ACP-OPS eu-west-2a 35.176.238.26 eu-west-2b 35.177.41.205 eu-west-2c 35.176.100.151 ACP-PROD eu-west-2a 35.177.82.170 eu-west-2b 52.56.249.216 eu-west-2c 35.176.70.245 ACP-PROD-PX eu-west-2a 35.177.156.249 eu-west-2b 35.177.245.179 eu-west-2c 35.177.14.177 ACP-PD-CBA-PX eu-west-2a 18.130.198.191 eu-west-2b 35.176.29.43 eu-west-2c 52.56.153.11 ACP-NOTPROD eu-west-2a 35.177.248.60 eu-west-2b 35.176.168.231 eu-west-2c 52.56.158.209 ACP-NOTPROD-PX eu-west-2a 52.56.221.144 eu-west-2b 52.56.45.52 eu-west-2c 35.177.167.246 ACP-NP-CBA-PX eu-west-2a 35.177.215.243 eu-west-2b 35.176.17.42 eu-west-2c 3.8.244.34 ACP-TEST eu-west-2a 35.176.184.49 eu-west-2b 35.176.217.238 eu-west-2c 35.177.169.118 ACP-TUNNEL eu-west-2a 52.56.221.216 eu-west-2b 18.130.11.142 eu-west-2c 18.130.6.5 Note: Your device's external IP address will only change when using the \"ACP-TUNNEL\" VPN profile. Other VPN profiles will create split tunnels that will not change your external IP address.","title":"ACP External IPs"},{"location":"external-addresses.html#acp-external-addresses","text":"The ACP clusters all run behind NAT gateways, with fixed external addresses. Note, unless you've specifically chosen to run in a specific availability zone, your pods can run in any of the AZs for that cluster, so all three addresses should be taken as the external IPs. Cluster Zone External Address ACP-CI eu-west-2a 52.56.254.215 eu-west-2b 35.176.205.192 eu-west-2c 35.178.14.117 ACP-OPS eu-west-2a 35.176.238.26 eu-west-2b 35.177.41.205 eu-west-2c 35.176.100.151 ACP-PROD eu-west-2a 35.177.82.170 eu-west-2b 52.56.249.216 eu-west-2c 35.176.70.245 ACP-PROD-PX eu-west-2a 35.177.156.249 eu-west-2b 35.177.245.179 eu-west-2c 35.177.14.177 ACP-PD-CBA-PX eu-west-2a 18.130.198.191 eu-west-2b 35.176.29.43 eu-west-2c 52.56.153.11 ACP-NOTPROD eu-west-2a 35.177.248.60 eu-west-2b 35.176.168.231 eu-west-2c 52.56.158.209 ACP-NOTPROD-PX eu-west-2a 52.56.221.144 eu-west-2b 52.56.45.52 eu-west-2c 35.177.167.246 ACP-NP-CBA-PX eu-west-2a 35.177.215.243 eu-west-2b 35.176.17.42 eu-west-2c 3.8.244.34 ACP-TEST eu-west-2a 35.176.184.49 eu-west-2b 35.176.217.238 eu-west-2c 35.177.169.118 ACP-TUNNEL eu-west-2a 52.56.221.216 eu-west-2b 18.130.11.142 eu-west-2c 18.130.6.5 Note: Your device's external IP address will only change when using the \"ACP-TUNNEL\" VPN profile. Other VPN profiles will create split tunnels that will not change your external IP address.","title":"ACP External Addresses"},{"location":"newuser.html","text":"New Users # Flow for new users # This document will go over the flow for getting access to our various services if you are finding difficulties with the specifics of the tasks outlined in this document please see the more detailed Developer Docs . Office 365 Account # Licensed Office 365 accounts should be requested from your Programme Management Office. We can request AD-only accounts to facilitate access to ACP services via Single Sign On as explained on the Support page Please note that to use SSO, which is using your Office 365 account to log onto other services, pop-ups will need to be enabled on your computer. VPN # VPN profiles are used to be able to get through to our multiple AWS environments as well as other data centers. The profiles can be downloaded from access which you can access using your Office 365 SSO. The following services can be only accessed when connected to the ACP Platform VPN profile. Platform Hub # The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The Hub also provides tools to develop, build, deploy and manage all your projects. Support Portal # All support requests, changes, incidents and announcements for ACP are managed via JIRA Service Desk . A set of request templates have been created to cover a wide majority of general requests that users normally need. If your issue / request type is not listed, there is a general request form available. Internally Accessible # Artifactory # Artifactory - Once you have an Office 365 account, you can sign in using the HOD SSO button. Requests for an Artifactory robot can be made on the ACP Support Portal . The docker repository in Artifactory is accessed via the docker.digital.homeoffice.gov.uk address. GitLab # GitLab - Once you have an Office 365 account, you can sign in using the Office 365 button. Kubernetes # You will need a Kubernetes user token before you can access any of the namespaces in the clusters. Follow the instructions here to get one for your teams namespaces: Getting a Kubernetes Token for the UK cluster Drone # We have two instances; One for GitHub and one for GitLab . Kibana # Kibana is accessible to ACP users via the default VPN profile. Sysdig # Sysdig is also accessible to ACP users via the default VPN profile and requests for specific team views can be made on the Support Portal . AWS Resources (S3 Buckets and RDS Instances) # If you require an S3 bucket or an RDS instance you can submit a support request on the Support Portal . Externally Accessible (for open source projects) # The following services can be access without a VPN connection and with personal accounts. GitHub # Github - To access our repositories you must have your personal Github invited to the UK Home Office organisation on Github. This is done during your first login to the Platform Hub. Please note that you will have to have your full name on your Github profile along with 2 Factor Authentication. If you don't have a personal account on Github, then you will need to create one. Quay # Quay - You can access Quay using your GitHub account. Once you have been added to the ukhomeoffice organisation on Quay, you can create repositories in the organisation. If you are not already part of the organisation you can submit a support request on the Support Portal to be added . You will also need a robot created for you to push to your repository once you have created one. Requests for a Quay robot can be made on the Support Portal .","title":"New User Guide"},{"location":"newuser.html#new-users","text":"","title":"New Users"},{"location":"newuser.html#flow-for-new-users","text":"This document will go over the flow for getting access to our various services if you are finding difficulties with the specifics of the tasks outlined in this document please see the more detailed Developer Docs .","title":"Flow for new users"},{"location":"newuser.html#office-365-account","text":"Licensed Office 365 accounts should be requested from your Programme Management Office. We can request AD-only accounts to facilitate access to ACP services via Single Sign On as explained on the Support page Please note that to use SSO, which is using your Office 365 account to log onto other services, pop-ups will need to be enabled on your computer.","title":"Office 365 Account"},{"location":"newuser.html#vpn","text":"VPN profiles are used to be able to get through to our multiple AWS environments as well as other data centers. The profiles can be downloaded from access which you can access using your Office 365 SSO. The following services can be only accessed when connected to the ACP Platform VPN profile.","title":"VPN"},{"location":"newuser.html#platform-hub","text":"The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The Hub also provides tools to develop, build, deploy and manage all your projects.","title":"Platform Hub"},{"location":"newuser.html#support-portal","text":"All support requests, changes, incidents and announcements for ACP are managed via JIRA Service Desk . A set of request templates have been created to cover a wide majority of general requests that users normally need. If your issue / request type is not listed, there is a general request form available.","title":"Support Portal"},{"location":"newuser.html#internally-accessible","text":"","title":"Internally Accessible"},{"location":"newuser.html#artifactory","text":"Artifactory - Once you have an Office 365 account, you can sign in using the HOD SSO button. Requests for an Artifactory robot can be made on the ACP Support Portal . The docker repository in Artifactory is accessed via the docker.digital.homeoffice.gov.uk address.","title":"Artifactory"},{"location":"newuser.html#gitlab","text":"GitLab - Once you have an Office 365 account, you can sign in using the Office 365 button.","title":"GitLab"},{"location":"newuser.html#kubernetes","text":"You will need a Kubernetes user token before you can access any of the namespaces in the clusters. Follow the instructions here to get one for your teams namespaces: Getting a Kubernetes Token for the UK cluster","title":"Kubernetes"},{"location":"newuser.html#drone","text":"We have two instances; One for GitHub and one for GitLab .","title":"Drone"},{"location":"newuser.html#kibana","text":"Kibana is accessible to ACP users via the default VPN profile.","title":"Kibana"},{"location":"newuser.html#sysdig","text":"Sysdig is also accessible to ACP users via the default VPN profile and requests for specific team views can be made on the Support Portal .","title":"Sysdig"},{"location":"newuser.html#aws-resources-s3-buckets-and-rds-instances","text":"If you require an S3 bucket or an RDS instance you can submit a support request on the Support Portal .","title":"AWS Resources (S3 Buckets and RDS Instances)"},{"location":"newuser.html#externally-accessible-for-open-source-projects","text":"The following services can be access without a VPN connection and with personal accounts.","title":"Externally Accessible (for open source projects)"},{"location":"newuser.html#github","text":"Github - To access our repositories you must have your personal Github invited to the UK Home Office organisation on Github. This is done during your first login to the Platform Hub. Please note that you will have to have your full name on your Github profile along with 2 Factor Authentication. If you don't have a personal account on Github, then you will need to create one.","title":"GitHub"},{"location":"newuser.html#quay","text":"Quay - You can access Quay using your GitHub account. Once you have been added to the ukhomeoffice organisation on Quay, you can create repositories in the organisation. If you are not already part of the organisation you can submit a support request on the Support Portal to be added . You will also need a robot created for you to push to your repository once you have created one. Requests for a Quay robot can be made on the Support Portal .","title":"Quay"},{"location":"rbac.html","text":"RBAC Groups # RBAC groups are used to control the level of access a user has to a namespace (or cluster). The RBAC groups that you need assign when creating a token will be dependant on the level of access needed for the user. Group names are created in the format: acp:<cluster-name>:<user-type>:[cw|ns]-<role-type>:<namespace> with cw|ns being cluster wide or namespace specific. Here is an example of an RBAC group: acp:notprod:robot:ns-robot:hello-world This indicates that this token: * is for the notprod cluster * is for a robot user (normal users would have user here instead) * is defined for this particular namespace * has a role type is robot which will give the user (which will most likely be a robot) that will use the token certain permissions in the namespace. Another role type is readonly which allows a user to list/get all of the resources in a namespace * will be used with a namespace called hello-world","title":"RBAC Guide"},{"location":"rbac.html#rbac-groups","text":"RBAC groups are used to control the level of access a user has to a namespace (or cluster). The RBAC groups that you need assign when creating a token will be dependant on the level of access needed for the user. Group names are created in the format: acp:<cluster-name>:<user-type>:[cw|ns]-<role-type>:<namespace> with cw|ns being cluster wide or namespace specific. Here is an example of an RBAC group: acp:notprod:robot:ns-robot:hello-world This indicates that this token: * is for the notprod cluster * is for a robot user (normal users would have user here instead) * is defined for this particular namespace * has a role type is robot which will give the user (which will most likely be a robot) that will use the token certain permissions in the namespace. Another role type is readonly which allows a user to list/get all of the resources in a namespace * will be used with a namespace called hello-world","title":"RBAC Groups"},{"location":"service-lifecycle.html","text":"Service Lifecycle # This defines the current service lifecycle stages. This isn't specific to the hosting platform and will no doubt move to somewhere more organistionally wide. However, this is a starting point to clearly communicate the phases of our evolution. Alpha # Alpha is an experimental phase to test the hypothesis, by building prototypes to validate the direction and the intention of the service. It is there to explore ways of achieving and meeting the user needs in the right way. There is more information on this here and here Beta # This is the phase where software is more feature complete. It is aimed at being functional and meeting the definition of a Minimal Viable Product, (MVP). The service is accessible and secured and meets a lot of the production requirements and needs. However, there will be an assessment of the service through closed user groups, to evaluate that it meets the user needs. SLA's, SLO's and availability may not meet the user expectation of a live service. Live # Once the products have gone through the beta process, (which is to identify that the MVP is viable), the service will transition to live. In live, we are happy that the service has undergone enough evaluation to meet user expectations as well as being able to commit confidentially to SLA's and SLO's. Production-Ready Criteria # To define what it means to be \"production-ready\", there is a set of criteria by which we assess services. This is not a concrete list and is likely to evolve. We would expect Beta and Live services to adhere to this criteria list, however, it is possible that a beta may decide that some are not necessary, depending on the size of the user group and the communication to said group. Generally speaking, it is best not to rely or depend on a Beta service. Resilient (Recover from Restart) Highly available (Multiple Instances) Backups of all data Validation of backup, (not 0 file size) Restores of data tested CI Release process Continuous deployment (if needed) to prod, (done via CI) Is it independent of itself i.e. it doesn't rely on itself Tests defined and ran as part of CI process Monitoring of product Does it have an ATO? Monitoring Dashboards produced Alerting in place and tested Patching Process Defined + Tested Intrusion detection in place Security tested Does it log adequately? Logs persisted Non root user if docker is used Readonly root filesystem and hardening of container Documentation in place and standards Default Admin Password Changed Dev instance / playground SSO where needed Change Management process defined Incident Management process defined (incl agreed SLAs) Monitoring of cert expiry","title":"Service Lifecycle"},{"location":"service-lifecycle.html#service-lifecycle","text":"This defines the current service lifecycle stages. This isn't specific to the hosting platform and will no doubt move to somewhere more organistionally wide. However, this is a starting point to clearly communicate the phases of our evolution.","title":"Service Lifecycle"},{"location":"service-lifecycle.html#alpha","text":"Alpha is an experimental phase to test the hypothesis, by building prototypes to validate the direction and the intention of the service. It is there to explore ways of achieving and meeting the user needs in the right way. There is more information on this here and here","title":"Alpha"},{"location":"service-lifecycle.html#beta","text":"This is the phase where software is more feature complete. It is aimed at being functional and meeting the definition of a Minimal Viable Product, (MVP). The service is accessible and secured and meets a lot of the production requirements and needs. However, there will be an assessment of the service through closed user groups, to evaluate that it meets the user needs. SLA's, SLO's and availability may not meet the user expectation of a live service.","title":"Beta"},{"location":"service-lifecycle.html#live","text":"Once the products have gone through the beta process, (which is to identify that the MVP is viable), the service will transition to live. In live, we are happy that the service has undergone enough evaluation to meet user expectations as well as being able to commit confidentially to SLA's and SLO's.","title":"Live"},{"location":"service-lifecycle.html#production-ready-criteria","text":"To define what it means to be \"production-ready\", there is a set of criteria by which we assess services. This is not a concrete list and is likely to evolve. We would expect Beta and Live services to adhere to this criteria list, however, it is possible that a beta may decide that some are not necessary, depending on the size of the user group and the communication to said group. Generally speaking, it is best not to rely or depend on a Beta service. Resilient (Recover from Restart) Highly available (Multiple Instances) Backups of all data Validation of backup, (not 0 file size) Restores of data tested CI Release process Continuous deployment (if needed) to prod, (done via CI) Is it independent of itself i.e. it doesn't rely on itself Tests defined and ran as part of CI process Monitoring of product Does it have an ATO? Monitoring Dashboards produced Alerting in place and tested Patching Process Defined + Tested Intrusion detection in place Security tested Does it log adequately? Logs persisted Non root user if docker is used Readonly root filesystem and hardening of container Documentation in place and standards Default Admin Password Changed Dev instance / playground SSO where needed Change Management process defined Incident Management process defined (incl agreed SLAs) Monitoring of cert expiry","title":"Production-Ready Criteria"},{"location":"services.html","text":"These are the core services which we provide across our platform. VPN # Many of our services are behind a VPN (using OpenVPN protocol) for security reasons. We have many different roles and profiles for accessing different environments. These profiles are all found at Access ACP . You can download VPN profiles for the environments you have access to. New profiles can be set up to connect through to things like project specific AWS accounts. Each profile will expire after a certain amount of time (8-72 hours), users will have to download a new profile once their certificates which are baked into the openvpn profiles expire. Source Code Management # GitHub # GitHub is where we store our open source code. You need to be added to the Uk HomeOffice organisation to access most of our code. More documentation can be found here GitLab # GitLab is where we store our more private code. Hosted in our ops cluster, you will need an office 365 account to get in via single sign on. Each project has its own group which are managed by members of that project. More in depth guides to gitlab can be found here . CI # Drone # Drone is our CI tool for building and deploying applications. We have two instances, one for GitLab and one for GitHub . More instructions can be found here . Jenkins # Jenkins is considered a legacy system and is not supported. Binary / Artefact Storage # Quay # Quay is where store all of our open source container images. ECR # We use AWS ECR to host our private container images for high availability and resiliency. For more information on how to use ECR see here Artifactory # We use Artifactory as our internal binary repository store where projects can push build time artefacts and dependencies. Prior to the introduction of ECR, Docker images were also pushed here and can be accessed via https://docker.digital.homeoffice.gov.uk. Please note that we regularly remove container images that have not been downloaded in a year. Domain Name System (DNS) Pattern # To standardise on how services route their application traffic to the appropriate hosting platform and to offer consistency in how we approach DNS we have a standard DNS naming convention for services. In parallel to this, users need to also be aware of the limits on certificates and letsencrypt if they are wanting external TLS certificates for their services. For Non-production Services # The following categories are something we would expect a service to specify: Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Env - The environment of the service i.e. Dev Service - The overall service name or project name i.e. example-service <servicename>.<env>.<service>-notprod.homeoffice.gov.uk web-portal.dev.example-service-notprod.homeoffice.gov.uk For Production Services # As we want to protect production services from hitting limits and to create a distinction between services that are non-production, (not prod) and production, we simplify the overall approach by using the main service name as the domain. Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Service - The overall service name or project name i.e. example-service <servicename>.<service>.homeoffice.gov.uk web-portal.example-service.homeoffice.gov.uk Application Composition # The following are containers that we create for use alongside your own application Keycloak Gatekeeper # Keycloak Proxy : a container for putting auth in front of your application. Nginx Proxy # Nginx Proxy : for TLS and proxying your application container. cert-manager # Cert-manager : for obtaining internal and external certificates. Logging # Logging stack consists of Elasticsearch , Logstash , Kibana ). Logstash agents deployed as a daemonSet will collect all workload logs and index them in Elasticsearch. Logs are searchable for a period of 5 days through Kibana UI . Access to view logs can be requested via Support request . Current Log Retention Policy # Logs are searchable in Kibana for 5 days and remain within Elasticsearch for 10 days. Collected workload logs will be persisted in S3 indefinitely and migrated to the infrequent access storage class and then glacier storage after 60 and 180 days respectively. NOTE: this may change in the future! The same policy applies to all logs within ELK Metrics / Monitoring # Sysdig # Sysdig is our metric collection tool. We are working closely with Sysdig on the development of this. It can be used for dashboards, alerting and much more. More information can be found on the Sysdig Monitor site and for the open-source tool, Sysdig (command line tool) and Sysdig Inspect . Sysdig training # Sysdig have an online training course which covers the basics of using the product, Sysdig 101 . Sysdig recommends new users complete this training. Security and disaster recovery # The Application Container Platform employs robust security principles, including but not limited to: encryption of data at rest and in transit restricting access to resources according to operational needs strict authorisation requirements for all endpoints role based access control The Platform is spread across multiple availability zones, which are essentially three different data centres within a region. In case of an entire AWS region going down for a prolonged period of time, the Platform can be recreated in another region within a few hours. The recovery of products hosted on the Platform are subject to considerations set out for the Production Ready criteria in Service Lifecycle . For further information on security and disaster recovery considerations, please raise a ticket on the Support Portal . Reusable components # Whilst building ACP, we've written a things that other projects may be interested in reusing. These can be found on GitHub here Terraform modules # Base docker images #","title":"Services"},{"location":"services.html#vpn","text":"Many of our services are behind a VPN (using OpenVPN protocol) for security reasons. We have many different roles and profiles for accessing different environments. These profiles are all found at Access ACP . You can download VPN profiles for the environments you have access to. New profiles can be set up to connect through to things like project specific AWS accounts. Each profile will expire after a certain amount of time (8-72 hours), users will have to download a new profile once their certificates which are baked into the openvpn profiles expire.","title":"VPN"},{"location":"services.html#source-code-management","text":"","title":"Source Code Management"},{"location":"services.html#github","text":"GitHub is where we store our open source code. You need to be added to the Uk HomeOffice organisation to access most of our code. More documentation can be found here","title":"GitHub"},{"location":"services.html#gitlab","text":"GitLab is where we store our more private code. Hosted in our ops cluster, you will need an office 365 account to get in via single sign on. Each project has its own group which are managed by members of that project. More in depth guides to gitlab can be found here .","title":"GitLab"},{"location":"services.html#ci","text":"","title":"CI"},{"location":"services.html#drone","text":"Drone is our CI tool for building and deploying applications. We have two instances, one for GitLab and one for GitHub . More instructions can be found here .","title":"Drone"},{"location":"services.html#jenkins","text":"Jenkins is considered a legacy system and is not supported.","title":"Jenkins"},{"location":"services.html#binary-artefact-storage","text":"","title":"Binary / Artefact Storage"},{"location":"services.html#quay","text":"Quay is where store all of our open source container images.","title":"Quay"},{"location":"services.html#ecr","text":"We use AWS ECR to host our private container images for high availability and resiliency. For more information on how to use ECR see here","title":"ECR"},{"location":"services.html#artifactory","text":"We use Artifactory as our internal binary repository store where projects can push build time artefacts and dependencies. Prior to the introduction of ECR, Docker images were also pushed here and can be accessed via https://docker.digital.homeoffice.gov.uk. Please note that we regularly remove container images that have not been downloaded in a year.","title":"Artifactory"},{"location":"services.html#domain-name-system-dns-pattern","text":"To standardise on how services route their application traffic to the appropriate hosting platform and to offer consistency in how we approach DNS we have a standard DNS naming convention for services. In parallel to this, users need to also be aware of the limits on certificates and letsencrypt if they are wanting external TLS certificates for their services.","title":"Domain Name System (DNS) Pattern"},{"location":"services.html#for-non-production-services","text":"The following categories are something we would expect a service to specify: Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Env - The environment of the service i.e. Dev Service - The overall service name or project name i.e. example-service <servicename>.<env>.<service>-notprod.homeoffice.gov.uk web-portal.dev.example-service-notprod.homeoffice.gov.uk","title":"For Non-production Services"},{"location":"services.html#for-production-services","text":"As we want to protect production services from hitting limits and to create a distinction between services that are non-production, (not prod) and production, we simplify the overall approach by using the main service name as the domain. Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Service - The overall service name or project name i.e. example-service <servicename>.<service>.homeoffice.gov.uk web-portal.example-service.homeoffice.gov.uk","title":"For Production Services"},{"location":"services.html#application-composition","text":"The following are containers that we create for use alongside your own application","title":"Application Composition"},{"location":"services.html#keycloak-gatekeeper","text":"Keycloak Proxy : a container for putting auth in front of your application.","title":"Keycloak Gatekeeper"},{"location":"services.html#nginx-proxy","text":"Nginx Proxy : for TLS and proxying your application container.","title":"Nginx Proxy"},{"location":"services.html#cert-manager","text":"Cert-manager : for obtaining internal and external certificates.","title":"cert-manager"},{"location":"services.html#logging","text":"Logging stack consists of Elasticsearch , Logstash , Kibana ). Logstash agents deployed as a daemonSet will collect all workload logs and index them in Elasticsearch. Logs are searchable for a period of 5 days through Kibana UI . Access to view logs can be requested via Support request .","title":"Logging"},{"location":"services.html#current-log-retention-policy","text":"Logs are searchable in Kibana for 5 days and remain within Elasticsearch for 10 days. Collected workload logs will be persisted in S3 indefinitely and migrated to the infrequent access storage class and then glacier storage after 60 and 180 days respectively. NOTE: this may change in the future! The same policy applies to all logs within ELK","title":"Current Log Retention Policy"},{"location":"services.html#metrics-monitoring","text":"","title":"Metrics / Monitoring"},{"location":"services.html#sysdig","text":"Sysdig is our metric collection tool. We are working closely with Sysdig on the development of this. It can be used for dashboards, alerting and much more. More information can be found on the Sysdig Monitor site and for the open-source tool, Sysdig (command line tool) and Sysdig Inspect .","title":"Sysdig"},{"location":"services.html#sysdig-training","text":"Sysdig have an online training course which covers the basics of using the product, Sysdig 101 . Sysdig recommends new users complete this training.","title":"Sysdig training"},{"location":"services.html#security-and-disaster-recovery","text":"The Application Container Platform employs robust security principles, including but not limited to: encryption of data at rest and in transit restricting access to resources according to operational needs strict authorisation requirements for all endpoints role based access control The Platform is spread across multiple availability zones, which are essentially three different data centres within a region. In case of an entire AWS region going down for a prolonged period of time, the Platform can be recreated in another region within a few hours. The recovery of products hosted on the Platform are subject to considerations set out for the Production Ready criteria in Service Lifecycle . For further information on security and disaster recovery considerations, please raise a ticket on the Support Portal .","title":"Security and disaster recovery"},{"location":"services.html#reusable-components","text":"Whilst building ACP, we've written a things that other projects may be interested in reusing. These can be found on GitHub here","title":"Reusable components"},{"location":"services.html#terraform-modules","text":"","title":"Terraform modules"},{"location":"services.html#base-docker-images","text":"","title":"Base docker images"},{"location":"developer-docs/index.html","text":"ACP Developer Documentation # Introduction # ACP serves as a platform for teams to build and deploy projects in the Home Office. In addition to other technologies that we use, we strongly recommend to get an understanding of two of the core technologies that ACP is based on - Docker and Kubernetes: Docker is a software tool designed to make it easier to create, deploy and run applications by packaging up along with all its dependencies to containers. Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure. For further information on Docker and Kubernetes: Docker Docker getting started tutorial Kubernetes What is Kubernetes? Kubernetes talk to introduce the concepts Developer getting started guide # Some prerequisites are required before developing on ACP. This guide will show you how to: Get access to the VPNs that allows you to connect to the platform Get setup with the kubectl client that lets you deploy applications to kubernetes Get access to Quay and Artifactory In addition, new developers should look at the new user flow documentation whilst going through this doc to serve as a checklist and make sure you are onboarded to all necessary platform services. Platform Hub # The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The hub also provides tools to develop, build, deploy and manage all your projects. Updates are also on its way to include more self-service tools, along with documentation, FAQs and live status updates. Access to the Platform Hub requires the Kube Platform VPN profile. Please make sure you have followed the Developer getting started guide for instructions on connecting to VPNs. Support Portal # All support requests, changes, incidents and announcements for ACP are managed via JIRA Service Desk . A set of request templates have been created to cover a wide majority of general requests that users normally need. If your issue / request type is not listed, there is a general request form available. Project getting started guide # Kubernetes resources (including your applications!) are always deployed into a particular namespace. These namespaces provide separation between the different projects hosted on the platform. A project service can have more than one namespace. For example, you can have namespaces that are different environments but are part of the same project service (e.g. dev and prod namespaces). Or if you have different namespaces that are all related to one project service (e.g. a web-api and an application namespace). For instructions on getting new namespaces and other relevant resources on getting started can be found below: ACP How-to Docs ACP How-to Docs # The How To Docs within the ACP repo provides a collection of how-to guides for both Developers and DevOps to use this Developer guide to Continuous Integration with Drone # Drone is what we use for Continuous Integration in ACP. This guide will cover how to use drone to test your PRs, build and push docker images, and to deploy. Writing Dockerfiles # This guide covers best practice for building Dockerfiles, and lists our standard base images.","title":"Developer Getting Started Guide"},{"location":"developer-docs/index.html#acp-developer-documentation","text":"","title":"ACP Developer Documentation"},{"location":"developer-docs/index.html#introduction","text":"ACP serves as a platform for teams to build and deploy projects in the Home Office. In addition to other technologies that we use, we strongly recommend to get an understanding of two of the core technologies that ACP is based on - Docker and Kubernetes: Docker is a software tool designed to make it easier to create, deploy and run applications by packaging up along with all its dependencies to containers. Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure. For further information on Docker and Kubernetes: Docker Docker getting started tutorial Kubernetes What is Kubernetes? Kubernetes talk to introduce the concepts","title":"Introduction"},{"location":"developer-docs/index.html#developer-getting-started-guide","text":"Some prerequisites are required before developing on ACP. This guide will show you how to: Get access to the VPNs that allows you to connect to the platform Get setup with the kubectl client that lets you deploy applications to kubernetes Get access to Quay and Artifactory In addition, new developers should look at the new user flow documentation whilst going through this doc to serve as a checklist and make sure you are onboarded to all necessary platform services.","title":"Developer getting started guide"},{"location":"developer-docs/index.html#platform-hub","text":"The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The hub also provides tools to develop, build, deploy and manage all your projects. Updates are also on its way to include more self-service tools, along with documentation, FAQs and live status updates. Access to the Platform Hub requires the Kube Platform VPN profile. Please make sure you have followed the Developer getting started guide for instructions on connecting to VPNs.","title":"Platform Hub"},{"location":"developer-docs/index.html#support-portal","text":"All support requests, changes, incidents and announcements for ACP are managed via JIRA Service Desk . A set of request templates have been created to cover a wide majority of general requests that users normally need. If your issue / request type is not listed, there is a general request form available.","title":"Support Portal"},{"location":"developer-docs/index.html#project-getting-started-guide","text":"Kubernetes resources (including your applications!) are always deployed into a particular namespace. These namespaces provide separation between the different projects hosted on the platform. A project service can have more than one namespace. For example, you can have namespaces that are different environments but are part of the same project service (e.g. dev and prod namespaces). Or if you have different namespaces that are all related to one project service (e.g. a web-api and an application namespace). For instructions on getting new namespaces and other relevant resources on getting started can be found below: ACP How-to Docs","title":"Project getting started guide"},{"location":"developer-docs/index.html#acp-how-to-docs","text":"The How To Docs within the ACP repo provides a collection of how-to guides for both Developers and DevOps to use this","title":"ACP How-to Docs"},{"location":"developer-docs/index.html#developer-guide-to-continuous-integration-with-drone","text":"Drone is what we use for Continuous Integration in ACP. This guide will cover how to use drone to test your PRs, build and push docker images, and to deploy.","title":"Developer guide to Continuous Integration with Drone"},{"location":"developer-docs/index.html#writing-dockerfiles","text":"This guide covers best practice for building Dockerfiles, and lists our standard base images.","title":"Writing Dockerfiles"},{"location":"developer-docs/dev-setup.html","text":"Developer setup guide # Introduction # This guide aims to prepare developers to use the Application Container Platform. You must complete the steps below before attending the ACP Induction. All examples in this document are for Linux distributions and instructions for other operating systems will vary. If you choose to use a Windows device, please ensure that Windows Subsystem for Linux is installed. Set up GovWifi credentials Office 365 Connecting to ACP VPN Platform Hub registration Add a ssh key to Gitlab Sign in to Sysdig Join our Slack instance Required binaries User agreement Connecting to GovWifi # Please refer to Connect to GovWifi to obtain your credentials and set up wireless internet access via GovWifi. Office 365 # Platform users must have Office 365 Single Sign-On (SSO) credentials for the digital.homeoffice.gov.uk domain. Please get in touch with your Programme Management Office to request an account or raise an issue on the Support Portal . If you can't access the Board, please ask a colleague to raise a request on your behalf. You will not be able to follow through the rest of this guide unless you have Office 365 credentials. Connecting to ACP VPN # Most of ACP's services operate behind a VPN which is accessible with an openvpn client. Instructions on installing openvpn for your OS can be found at OpenVPN Once you've got your Office 365 SSO credentials, you can now navigate to Remote Access and login with your Office 365 account by clicking on the link on the right. Please download the VPN profile named \"Kube Platform\" and use the openvpn client to connect. Verify that you can resolve the Platform Hub before continuing on. VPN profiles expire after 72 hours. You'll need to download and connect with a new VPN Profile when it expires. Platform Hub registration # Please note that you need to have your VPN running to access the Platform Hub and also talk to cluster APIs. You will need to register with the Platform Hub in order to gain access tokens for our Kubernetes clusters. Head to Platform Hub , you will need your O365 credentials to login/sign up. You will be asked to connect your Github account to your Platform Hub account. This will give you access to project repositories under the UKHomeOffice Organisation in GitHub. Navigate to the Support Portal (JIRA Service Desk), logging in via your O365 account, and create a support request for access to ACP Induction . The support request will be sent to and reviewed by a member of the ACP team. Any updates on the request will be available to view within the Support Portal and additionally emailed to you. Please note that if you have an AD-only email account (a digital account without a mailbox), then you can raise a support request to update your email address so that you can receive email notifications on ticket updates. If you find that you cannot raise support requests, this may be because you have not been added to a team yet. Please ask one of your team members who already has access to the support portal to raise a request to have you added to service desk . Once created, your token will be shown in the Platform Hub under the Kubernetes section of the Connected Identities tab on the sidebar. You can view all of your tokens by pressing the Show Tokens button. Add a ssh key to Gitlab # You will need to add a ssh public key to your Gitlab profile before attending the Induction. Please sign into Gitlab with Office 365 and add a ssh public key to your profile. Instructions for generating a ssh keypair can be found in the Gitlab Docs . Sign in to Sysdig # Platform users with a digital account can sign into our Sysdig instance . Just click the OpenID button and then the O365 button. Initially you will be added to a default team that does not have access to any data, however you will be added to the ACP Induction team on (or shortly before) the day. Join our Slack instance # Platform users can join our Slack instance using a Home Office email address. If you do not already have an account on our instance, you can create an account here . Please note that the sign up process involves clicking a link that will be emailed to you, so if you only have an AD-only digital account (i.e. if your digital email does not have a mailbox), you will need to raise a request to have your corporate email invited to Slack. Once you have an account on Slack, please join the #acp-support and #acp-induction channels. Other useful channels include #acp-service-alerts for the current status of ACP services (which can also be found on our status page ) and #acp-feedback where you can provide feedback regarding our platform. Required binaries # Before joining the ACP Induction, we kindly ask you to install binaries which are used for the deployment of applications in ACP - instructions on how to install these are shown below: Git Docker Drone Kubectl AWS CLI Install Git # Verify if you have Git installed by: $ git --version git version 2.16.2 If Git is not installed, instructions on how to download and install it can be found over at the Git website . Please note that we assume a basic knowledge of Git for the ACP Induction. If you've not used git before, or need to brush up on your skills please see Git basics . Install Docker # You can follow the instructions to install docker from the Docker website . You can verify the installation is successful with: $ docker --version Docker version 18.03.1-ce, build 9ee9f40 # version 17.06 or above is required Install Drone # Drone CLI can be downloaded from the Drone CI website . Instructions installing on multiple operating systems are shown on the webpage. Currently our Drone server instance is compatible with version 0.8.6 , which can be found here or here . Verify that the installation is successful: $ drone --version drone version 0.8.6 Please note that Drone CLI version 1.0 and above are not fully compatible with our version of the Drone server. Please install v0.8.6 . Install Kubectl # You can follow the instructions to install version v1.13.4 of kubectl from the Kubernetes website . You can verify the installation is successful with: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4\", GitCommit:\"c27b913fddd1a6c480c229191a087698aa92f0b1\", GitTreeState:\"clean\", BuildDate:\"2019-02-28T13:37:52Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"} Install AWS CLI # Follow the official AWS documentation for instructions to install the AWS CLI. You can verify the installation is successful with: $ aws --version aws-cli/1.16.28 Python/2.7.10 Darwin/18.2.0 botocore/1.12.18 # version 1.11.91 or above is required Connecting to the cluster # In order to access a namespace you will need to configure kubectl to connect to our clusters - instructions on setting it up can be found on the Set up Kube Config button on the Connected Identities page. Verify that your kubectl is configured properly by trying to list pods and secrets in the acp-induction namespace: $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get pods No resources found. $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get secrets NAME TYPE DATA AGE default-token-dcnmg kubernetes.io/service-account-token 3 105d The output of the command above may differ if there are other pods or extra secrets deployed to the namespace. User agreement # Finally, please head over and read through our support documentation to familiarise yourself with the level of service ACP provides to its users including the level and hours of support on offer and issue escalation procedures.","title":"Developer setup guide"},{"location":"developer-docs/dev-setup.html#developer-setup-guide","text":"","title":"Developer setup guide"},{"location":"developer-docs/dev-setup.html#introduction","text":"This guide aims to prepare developers to use the Application Container Platform. You must complete the steps below before attending the ACP Induction. All examples in this document are for Linux distributions and instructions for other operating systems will vary. If you choose to use a Windows device, please ensure that Windows Subsystem for Linux is installed. Set up GovWifi credentials Office 365 Connecting to ACP VPN Platform Hub registration Add a ssh key to Gitlab Sign in to Sysdig Join our Slack instance Required binaries User agreement","title":"Introduction"},{"location":"developer-docs/dev-setup.html#connecting-to-govwifi","text":"Please refer to Connect to GovWifi to obtain your credentials and set up wireless internet access via GovWifi.","title":"Connecting to GovWifi"},{"location":"developer-docs/dev-setup.html#office-365","text":"Platform users must have Office 365 Single Sign-On (SSO) credentials for the digital.homeoffice.gov.uk domain. Please get in touch with your Programme Management Office to request an account or raise an issue on the Support Portal . If you can't access the Board, please ask a colleague to raise a request on your behalf. You will not be able to follow through the rest of this guide unless you have Office 365 credentials.","title":"Office 365"},{"location":"developer-docs/dev-setup.html#connecting-to-acp-vpn","text":"Most of ACP's services operate behind a VPN which is accessible with an openvpn client. Instructions on installing openvpn for your OS can be found at OpenVPN Once you've got your Office 365 SSO credentials, you can now navigate to Remote Access and login with your Office 365 account by clicking on the link on the right. Please download the VPN profile named \"Kube Platform\" and use the openvpn client to connect. Verify that you can resolve the Platform Hub before continuing on. VPN profiles expire after 72 hours. You'll need to download and connect with a new VPN Profile when it expires.","title":"Connecting to ACP VPN"},{"location":"developer-docs/dev-setup.html#platform-hub-registration","text":"Please note that you need to have your VPN running to access the Platform Hub and also talk to cluster APIs. You will need to register with the Platform Hub in order to gain access tokens for our Kubernetes clusters. Head to Platform Hub , you will need your O365 credentials to login/sign up. You will be asked to connect your Github account to your Platform Hub account. This will give you access to project repositories under the UKHomeOffice Organisation in GitHub. Navigate to the Support Portal (JIRA Service Desk), logging in via your O365 account, and create a support request for access to ACP Induction . The support request will be sent to and reviewed by a member of the ACP team. Any updates on the request will be available to view within the Support Portal and additionally emailed to you. Please note that if you have an AD-only email account (a digital account without a mailbox), then you can raise a support request to update your email address so that you can receive email notifications on ticket updates. If you find that you cannot raise support requests, this may be because you have not been added to a team yet. Please ask one of your team members who already has access to the support portal to raise a request to have you added to service desk . Once created, your token will be shown in the Platform Hub under the Kubernetes section of the Connected Identities tab on the sidebar. You can view all of your tokens by pressing the Show Tokens button.","title":"Platform Hub registration"},{"location":"developer-docs/dev-setup.html#add-a-ssh-key-to-gitlab","text":"You will need to add a ssh public key to your Gitlab profile before attending the Induction. Please sign into Gitlab with Office 365 and add a ssh public key to your profile. Instructions for generating a ssh keypair can be found in the Gitlab Docs .","title":"Add a ssh key to Gitlab"},{"location":"developer-docs/dev-setup.html#sign-in-to-sysdig","text":"Platform users with a digital account can sign into our Sysdig instance . Just click the OpenID button and then the O365 button. Initially you will be added to a default team that does not have access to any data, however you will be added to the ACP Induction team on (or shortly before) the day.","title":"Sign in to Sysdig"},{"location":"developer-docs/dev-setup.html#join-our-slack-instance","text":"Platform users can join our Slack instance using a Home Office email address. If you do not already have an account on our instance, you can create an account here . Please note that the sign up process involves clicking a link that will be emailed to you, so if you only have an AD-only digital account (i.e. if your digital email does not have a mailbox), you will need to raise a request to have your corporate email invited to Slack. Once you have an account on Slack, please join the #acp-support and #acp-induction channels. Other useful channels include #acp-service-alerts for the current status of ACP services (which can also be found on our status page ) and #acp-feedback where you can provide feedback regarding our platform.","title":"Join our Slack instance"},{"location":"developer-docs/dev-setup.html#required-binaries","text":"Before joining the ACP Induction, we kindly ask you to install binaries which are used for the deployment of applications in ACP - instructions on how to install these are shown below: Git Docker Drone Kubectl AWS CLI","title":"Required binaries"},{"location":"developer-docs/dev-setup.html#install-git","text":"Verify if you have Git installed by: $ git --version git version 2.16.2 If Git is not installed, instructions on how to download and install it can be found over at the Git website . Please note that we assume a basic knowledge of Git for the ACP Induction. If you've not used git before, or need to brush up on your skills please see Git basics .","title":"Install Git"},{"location":"developer-docs/dev-setup.html#install-docker","text":"You can follow the instructions to install docker from the Docker website . You can verify the installation is successful with: $ docker --version Docker version 18.03.1-ce, build 9ee9f40 # version 17.06 or above is required","title":"Install Docker"},{"location":"developer-docs/dev-setup.html#install-drone","text":"Drone CLI can be downloaded from the Drone CI website . Instructions installing on multiple operating systems are shown on the webpage. Currently our Drone server instance is compatible with version 0.8.6 , which can be found here or here . Verify that the installation is successful: $ drone --version drone version 0.8.6 Please note that Drone CLI version 1.0 and above are not fully compatible with our version of the Drone server. Please install v0.8.6 .","title":"Install Drone"},{"location":"developer-docs/dev-setup.html#install-kubectl","text":"You can follow the instructions to install version v1.13.4 of kubectl from the Kubernetes website . You can verify the installation is successful with: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4\", GitCommit:\"c27b913fddd1a6c480c229191a087698aa92f0b1\", GitTreeState:\"clean\", BuildDate:\"2019-02-28T13:37:52Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}","title":"Install Kubectl"},{"location":"developer-docs/dev-setup.html#install-aws-cli","text":"Follow the official AWS documentation for instructions to install the AWS CLI. You can verify the installation is successful with: $ aws --version aws-cli/1.16.28 Python/2.7.10 Darwin/18.2.0 botocore/1.12.18 # version 1.11.91 or above is required","title":"Install AWS CLI"},{"location":"developer-docs/dev-setup.html#connecting-to-the-cluster","text":"In order to access a namespace you will need to configure kubectl to connect to our clusters - instructions on setting it up can be found on the Set up Kube Config button on the Connected Identities page. Verify that your kubectl is configured properly by trying to list pods and secrets in the acp-induction namespace: $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get pods No resources found. $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get secrets NAME TYPE DATA AGE default-token-dcnmg kubernetes.io/service-account-token 3 105d The output of the command above may differ if there are other pods or extra secrets deployed to the namespace.","title":"Connecting to the cluster"},{"location":"developer-docs/dev-setup.html#user-agreement","text":"Finally, please head over and read through our support documentation to familiarise yourself with the level of service ACP provides to its users including the level and hours of support on offer and issue escalation procedures.","title":"User agreement"},{"location":"how-to-docs/index.html","text":"How To's # This tree contains a collection of how-to guides for Developers. Create an Artifactory access token # Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please raise the relevant support request via the Support Portal . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u<username>:<api-key> -XPOST \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token\" -d \"username=<robot-username>\" -d \"scope=member-of-groups:<appropriate-groups>\" -d \"expires_in=0\" where <robot-username> is the name of the access token and <appropriate-groups> is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section. Kubernetes Pod Autoscaling # For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100 Chisel # The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination. A Working Example # We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:v1.3.1 # Both Chisel Client & Server versions must match securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point. Debug Issues with your deployments # Debug with secrets # Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set. Debugging issues with your deployments to the platform # If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application. Debugging deployments # We suggest the following steps: 1. Check your deployment, replicaset and pods created properly # $ kubectl get deployments $ kubectl get rs $ kubectl get pods 2. Investigate potential issues with your pods (this is most likely) # If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.* 3. Investigate potential issues with your service # A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name 4. Investigate potential issues with ingress # Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti <ingress-3879072234-5f4uq> -c <proxy> bash where <proxy> is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs <ingress-3879072234-5f4uq> DMS Migration # Prerequisite # The following need to be true before you follow this guide: * AWS console logon * Access to the DMS service from console * A region where STS has been activated DMS Setup # Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button. Downscaling Services Out Of Hours # In an effort to reduce costs on running the platform, we've enabled the capability to scale down specific resources Out Of Hours (OOH) for Non-Production and Production environments. AWS RDS (Relational Database Service) # RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Support Portal support request template . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage. Kubernetes Pods # Automatically scale down Kubernetes Deployments & Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments & Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters. Usage Set ONE of the following annotations on your Deployment / Statefulset: - downscaler/uptime : A time schedule in which the Deployment should be scaled up - downscaler/downtime : A time schedule in which the Deployment should be scaled down to 0 replicas The annotation values for the timeframe must have the following format to be processed correctly: <WEEKDAY-FROM>-<WEEKDAY-TO-INCLUSIVE> <HH>:<MM>-<HH>:<MM> <TIMEZONE> For example, to schedule a Deployment to only run on weekdays during working hours, the following annotation would be set: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London Note: When the deployment is downscaled, an additional annotation downscaler/original-replicas is automatically set to retain a history of the desired replicas prior to the downscale action. If this annotation has been deleted before the service is automatically scaled back up, the downscaler service will not know what to set the replicas back to, and so it won't attempt to scale up the resource. Example Spec: apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London labels: name: example-app name: example-app namespace: acp-example spec: replicas: 2 template: spec: containers: image: docker.digital.homeoffice.gov.uk/acp-example-app:v0.0.1@sha256:07397c41ac25c4b19e0485006849201f04168703f0016fad75b8ba5d9885d6d4 ... Drone How To # Install Drone CLI # Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN=<your_drone_token> If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly. Activate your pipeline # Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Sync your repository access rights with Drone by clicking the icon in the top right corner again with the Synchronize button - this needs to be applied everytime when a new repository is created. Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo. Configure your pipeline # In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name <...> with the name of your app. You should be able to watch your build succeed in the Drone UI. Publishing Docker images # Publishing to Quay If your repository is hosted on Gitlab, you don't want to publish your images to Quay. Images published to Quay are public and can be inspected and downloaded by anyone. You should publish your private images to Artifactory . Register for a free Quay account using your Github account linked to the Home Office organisation. Once you've logged into Quay check that you have ukhomeofficedigital under Users and Organisations. If you do not, submit a support request on the support portal for access to the ukhomeoffice organisation . Once you have access to view the ukhomeofficedigital repositories, click repositories and click the + Create New Repositories that is: public empty - no need to create a repo from a Dockerfile or link it to an existing repository Add your project to the UKHomeOffice Quay account and submit a support request on the support portal for a new Quay robot . Add the step to publish the docker image to Quay in your Drone pipeline: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where <your_quay_repo> in: quay.io/ukhomeofficedigital/<your_quay_repo> is the name of the Quay repo you (should) have already created. Note: ${DRONE_COMMIT_SHA} is a Drone environment variable that is passed to the container at runtime. The build should fail with the following error: Error response from daemon: Get https://quay.io/v2/: unauthorized: Could not find robot with username: ukhomeofficedigital+<your_robot_username> and supplied password. The error points to the missing password for the Quay robot. You will need to add this as a drone secret. You can do this through the Drone UI by going to your repo, clicking the menu icon in the top right and then clicking Secrets . You should be presented with a list of the secrets for that repo (if there are any) and you should be able to add secrets giving them a name and value. Add a secret with the name DOCKER_PASSWORD and with the value being the robot token that was supplied to you. Alternatively, you can use the Drone CLI to add the secret: $ drone secret add --repository ukhomeoffice/<your_github_repo> --name DOCKER_PASSWORD --value <your_robot_token> Restarting the build should be enough to make it pass. The Drone CLI allows for more control over the secret as opposed to the UI. For example, the CLI allows you to specify the image and the events that the secret will be allowed to be used with. Also note that the secret was specified in the secrets section of the pipeline to give it access to the secret. Without this, the pipeline would not be able to use the secret and it would fail. Secrets in this section are automatically uppercased at runtime so it is important that the secret is uppercased in your commands. You can also push specifically tagged images by using the DRONE_TAG Drone environment variable and by using the tag event: tagged_image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - ${DRONE_TAG} when: event: tag Tag using git tag v1.0 and push your tag with git push origin v1.0 (replace v1.0 with the tag you actually want to use). Note: These pipeline configurations are using the Docker plugin for Drone. For more information, see http://plugins.drone.io/drone-plugins/drone-docker/ Publishing to Artifactory Images hosted on Artifactory are private. If your repository is hosted publicly on GitHub, you shouldn't publish your images to Artifactory. Artifactory is only used to publish private images. You should use Quay to publish your public images . Submit a support request for a new Artifactory access token . You should be supplied an access token in response. You can inject the token that has been supplied to you with: $ drone secret add --repository <gitlab_repo_group>/<your_gitlab_repo> --name DOCKER_PASSWORD --value <your_robot_token> You can add the following step in your .drone.yml : image_to_artifactory: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=<your_robots_username> registry: docker.digital.homeoffice.gov.uk repo: docker.digital.homeoffice.gov.uk/<your_artifactory_repo> tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where the <image_name> in: docker tag <image_name> docker.digital.homeoffice.gov.uk/ukhomeofficedigital/<your_artifactory_repo>:$${DRONE_COMMIT_SHA} is the name of the image you tagged previously in the build step. The image should now be published on Artifactory. Please note that we regularly remove container images that have not been downloaded in a year. Deployments # Deployments and promotions Create a step that runs only on deployments: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment Push the changes to your remote repository. You can deploy the build you just pushed with the following command: $ drone deploy ukhomeoffice/<your_repo> 16 preprod Where 16 is the successful build number on drone that you wish to deploy to the preprod environment. You can pass additional parameters to your deployment as environment variables: $ drone deploy ukhomeoffice/<your_repo> 16 preprod -p DEBUG=1 -p NAME=Dan and use them in the step like this: deploy-to-preprod: image: busybox commands: - /bin/echo hello $${NAME} when: environment: preprod event: deployment Environments are strings and can be set to any value. When you wish to deploy to several environments you can create a step for each one of them: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment deploy-to-prod: image: busybox commands: - /bin/echo hello prod when: environment: prod event: deployment And deploy them accordingly: $ drone deploy ukhomeoffice/<your_repo> 16 preprod $ drone deploy ukhomeoffice/<your_repo> 16 prod Read more on environments . Drone as a Pull Request builder Drone pipelines are triggered when events occurs. Event triggers can be as simple as a push , a tagged commit , a pull request or as granular as only for pull requests for a branch named test . You can limit the execution of build steps at runtime using the when block. As an example, this block executes only on pull requests: pr-builder: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: event: pull_request Drone will only execute that step when a new pull request is raised (and when pushes are made to the branch while a pull request is open). Read more about Drone conditions . Deploying to ACP Please note that this section assumes that you already have kube files to work with (specifically, deployment, service and ingress files). Examples of these files can be found in the kube-signed-commit-check project. Add a deployment script with the following: #!/bin/bash export KUBE_NAMESPACE=<dev-induction> export KUBE_SERVER=${KUBE_SERVER} export KUBE_TOKEN=${KUBE_TOKEN} kd -f deployment.yaml \\ -f service.yaml \\ -f ingress.yaml Please note that this is only an example script and it will need to be changed to fit your particular application's needs. If you deployed this now you would likely receive an error similar to this: error: You must be logged in to the server (the server has asked for the client to provide credentials) This error appears because kd needs 3 environment variables to be set before deploying: KUBE_NAMESPACE - The kubernetes namespace you wish to deploy to. You need to provide the kubernetes namespace as part of the deployment job . KUBE_TOKEN - This is the token used to authenticate against the kubernetes cluster. If you do not already have a kube token, here are docs explaining how to get one . KUBE_SERVER - This is the address of the kubernetes cluster that you want to deploy to. You will need to add KUBE_TOKEN and KUBE_SERVER as drone secrets. Information about how to add Drone secrets can be found in the publishing to Quay section . You can verify that the secrets for your repo are present with: $ drone secret ls --repository ukhomeoffice/<your-repo> Once the secrets have been added, add a new step to your drone pipeline that will execute the deployment script: deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - ./deploy.sh when: environment: uat event: deployment Using Another Repo # It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/<deployment_repo> branch: <master> deploy_to: <uat> params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/<your_repo> 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs. Versioned deployments # When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update && apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment Migrating your pipeline # Secrets and Signing It is no longer necessary to sign your .drone.yml so the .drone.yml.sig can be deleted. Secrets can be defined in the Drone UI or using the CLI. Secrets created using the UI will be available to push, tag and deployment events. To restrict to selected events, or to allow pull request builds to access secrets you must use the CLI. Pipelines by default do not have access to any Drone secrets that you have added. You must now define which secrets a pipeline is allowed access to in a secrets section in your pipeline. Here is an example of a pipeline that has access to the DOCKER_PASSWORD secret which will be used to push an image to Quay: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - latest when: branch: master event: push Note: Secrets names in the secrets section will have their names uppercased at runtime. Organisation secrets are no longer available. This means that if you are using any organisation secrets such as KUBE_TOKEN_DEV , you will need to add a secret in Drone to replace it. Docker-in-Docker # The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push Services # If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306 Variable Escaping # Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} --> $${DOCKER_PASSWORD} ${DRONE_TAG} --> $${DRONE_TAG} ${DRONE_COMMIT_SHA} --> $${DRONE_COMMIT_SHA} Scanning Images in Drone # ACP provides Anchore as a scanning solution for images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well as any public images. Example pipeline: pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables) image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # Indicates the image is locally available local_image: true # This indicates we are willing tolerate any vulnerabilities which are below medium (valid values: negligible, low, medium, high, critical) tolerate: medium # An optional whitelist (comma separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: <PATH> # Indicates we should show all vulnerabilities regardless show_all_vulnerabilities: false # By default the plugin will exit will fail if any vulnerabilities are discovered which are not tolerated, # you change this behaviour by setting the below fail_on_detection: false Q&As # Q: The build fails with \"ERROR: Insufficient privileges to use privileged mode\" A: Remove privileged: true from your .drone.yml . As explained in the migrating your pipeline section , the primary use of this was for Docker-in-Docker which is not required. Q: The build fails with \"Cannot connect to the Docker daemon. Is the docker daemon running on this host?\" A: Make sure that your steps contain the environment variable DOCKER_HOST=tcp://172.17.0.1:2375 like in this case: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Q: The build fails when uploading to Quay with the error \"Error response from daemon: Get https://quay.io/v2/: unauthorized:...\" A: This is likely because the secret wasn't added correctly or the password is incorrect. Check that the secret has been added to Drone and that you have added the secrets section in your .drone.yaml it to the pipeline that requires it. Q: As part of my build process I have two Dockerfiles to produce a Docker image. How can I share files between builds in the same step? A: When the pipeline starts, Drone creates a Docker data volume that is passed along all active steps in the pipeline. If the first step creates a test.txt file, the second step can use that file. As an example, this pipeline uses a two step build process: pipeline: first-step: image: busybox commands: - echo hello > test.txt when: branch: master event: push second-step: image: busybox commands: - cat test.txt when: branch: master event: push Q: Should I use Gitlab with Quay? A: Please don't. If your repository is hosted in Gitlab then use Artifactory to publish your images. Images published to Artifactory are kept private. If you still want to use Quay, you should consider hosting your repository on the open (Github). Q: Can I create a token that has permission to create ephemeral/temporary namespaces? A: No. This is because there is currently no way to give access to namespaces via regex. I.e. There is no way to give access to any namespace with the format: my-temp-namespace-* (where * would be build number or something similar). Alternatively, you can be given a named namespace in the CI cluster. Please create an issue on our Support Portal if you require this. AWS ECR for Private Docker Images # AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials. Creating a Docker Repository # Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates. Generating Access Credentials # Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it. Accessing a Docker Repository # Accessing the AWS Container Registry to Pull & Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated. Pre-Requisites # To follow the below steps you must have: * AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows * Docker (version 17.06 or above, check with docker --version ) Step 1: Retrieve an authorisation token # Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com Step 2: Login with Authorisation Token # Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Steps 1 and 2 will also not work if you are using AWS CLI (version 2.*). Instead use $ aws_account_id=\"340268328991\" $ aws_region=\"eu-west-2\" $ ecr_url=\"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" $ aws --region \"${aws_region}\" ecr get-login-password \\ | docker login \\ --password-stdin \\ --username AWS \\ \"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" Login Succeeded Pulling & Pushing Images # Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Listing Images & Housekeeping # Using the AWS CLI you can list all images that have been pushed to a given repository which you have access to. For example: $ aws ecr list-images --repository-name acp/hello-world-app { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" }, { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ] } To delete old images, you must have write access enabled or perform the action via a Robot Account. Images can be deleted based on a provided tag or digest. When providing a digest, all image tags with the same digest are deleted together. $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageTag=v0.0.1 { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" } ], \"failures\": [] } $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageDigest=sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ], \"failures\": [] } Managing Image Deployments via Drone CI # The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI. Using Ingress # An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: \"nginx-internal\" # Indicate the ingress SHOULD speak TLS between itself and pods (best-practice) ingress.kubernetes.io/backend-protocol: \"HTTPS\" name: myapp-server-internal spec: rules: - host: \"myapp.myproject.homeoffice.gov.uk\" http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - \"myapp.myproject.homeoffice.gov.uk\" # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Please view the official documentation for a full list of available ingress-nginx annotations . Note: Where the prefix for the annotation in the docs references nginx.ingress.kubernetes.io/ , this should be changed to ingress.kubernetes.io/ when running within ACP (as per the above example). Cert Manager # VERY IMPORTANT upgrade information # cert-manager is being upgraded from v0.8 to v0.13.1. If you have cert-manager resources deployed in your namespaces, you MUST follow the instructions to upgrade from v0.8 to upgrade annotations and labels in order for them to be managed by the new version of cert-manager. To find out if you are using v0.8 cert-manager resources in your namespace, you can run: kubectl get certificates.certmanager.k8s.io Also LetsEncrypt will no longer be supporting PSG's kube-cert-manager from June 2020. So if you are using PSG kube-cert-manager to obtain certificates for your ingresses, you also need to migrate to JetStack's cert-manager v0.13.1 and follow the instructions to upgrade from PGS's kube-cert-manager To find out if you are using PSG kube-cert-manager to manage your ingresses certificates, you can run: kubectl get ingresses -o yaml | grep stable.k8s.psg.io Please also be aware that admission policies have been updated and will reject Ingress resources with annotations or labels supported by more than one certificate manager. There are currently ingresses with both cert-manager v0.8 and PSG annotations or labels and those will now fail applying. Background # The ACP platform presently has two certificate management services. The first service was PSG's kube-cert-manager . However with the forever changing landscape the project gradually became deprecated and now recommends replacement with JetStack's cert-manager . Therefore, projects still using kube-cert-manager should modify their services to start using cert-manager instead. Note that ACP will continue to support kube-cert-manager and the internal cfssl service while they are still in use, but we do recommend shifting over to cert-manager as soon as possible as aside from security fixes there will not be any more updates to these services. Without wishing to duplicate documentation which can be found in the readme and or official documentation , cert-manager can effectively replace two services: kube-cert-manager: used to acquire certificates from LetsEncrypt. cfssl: an internal Cloudflare service used to generate internal certificate (usually to encrypt between ingress and pod) . IMPORTANT NOTE: cert-manager is being upgraded from v0.8 to v0.13.1. In order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io How-tos # As a developer I already have a certificate from the legacy kube-cert-manager, how do I migrate? # Migrating from the former kube-cert-manager over to cert-manager means creating the certificate request as below and removing the annotations from the ingress. However, the safe way would be to; Create a new Certificate resource and point to a new secret name (thus keeping the old one incase) . Push out the change and wait for the certificate to be fulfilled. Once you have the certificate you can update your ingress to use the new secret, remove the annotations and use the Certificate resource thereafter. As a developer I want to retrieve an internal certificate # As stated above the cert-manager can also handle internal certificates i.e. those signed by the internal ACP Certificate Authority (this is self signed btw) . At the moment you might be using cfssl-sidekick to perform this, but this can be completely replaced. If you want to create a certificate for a service, assuming the service is called myservice in namespace mynamespace , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Ingress resources are checked by admission policies to ensure the platform-ca cluster issuer only issues certificates for DNS names that are hosted inside the namespace. So if you want to create a certificate for a replica in a statefulset, assuming your statufelset is called mysts , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - mysts-0.myservice.mynamespace - mysts-0.myservice.mynamespace.svc - mysts-0.myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Note that mysts-0.myservice is intentionally missing from the list in dnsNames because those names need to be either a hostname (for the service) or a name ending with mynamespace , mynamespace.svc or mynamespace.svc.cluster.local . This would create a kubernetes secret named tls in your namespace with the signed certificate. An interesting thing to note here is that although this is using the ClusterIssuer platform-ca created by the ACP team, there is nothing stopping a project from creating a local Issuer for their own project. So for example. --- apiVersion: cert-manager.io/v1alpha2 kind: Issuer metadata: name: project-ca spec: ca: secretName: project-ca --- apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: project-ca # @Note: we have change from ClusterIssuer to a local Issuer kind: Issuer commonName: site.svc.project.cluster.local dnsNames: - localhost ipAddresses: - 127.0.0.1 Finally, if you want to use your certificate for client auth (as well as server auth in the following example), you need to add a keyUsages section to your Certificate resource: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer keyUsages: - server auth - client auth dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 As a developer I want to retrieve a certificate for my external service # Let's assume we have an externally facing site which we wish to expose via ingress and we want a valid LetsEncrypt certificate. Getting a certificate associated with the external ingress only requires to annotate the ingress with cert-manager.io/enabled , which is a toggle to ask cert-manager to handle this ingress resource. Optionally, the acme solver to be used by the cluster issuer can be specified with label cert-manager.io/solver: http01 . However, this is not required as the http01 acme solver is the default one. Please note that cert-manager.io/enabled is an annotation but cert-manager.io/solver is a label. When the site is externally facing i.e. the ingress class on the ingress is kubernetes.io/ingress.class: nginx-external you should always default to using a http01 challenge. However, if you know that the domain whitelisted in your namespace is hosted in AWS by Route53, you can instead specify a label of cert-manager.io/solver: route53 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: # @NOTE: this will enable cert-manager to handle this resource cert-manager.io/enabled: \"true\" ingress.kubernetes.io/affinity: cookie ingress.kubernetes.io/force-ssl-redirect: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/session-cookie-name: ingress ingress.kubernetes.io/ssl-redirect: \"true\" kubernetes.io/ingress.class: nginx-external name: example # @NOTE: the following label can be specified to ask letsencrypt to use the http01 acme challenge # @NOTE: but it is not required as http01 is the default solver labels: cert-manager.io/solver: http01 spec: rules: - host: www.example.com http: paths: - backend: serviceName: service_name servicePort: 10443 path: / tls: - hosts: - www.example.com # @NOTE: this is the name of the kubernetes secret that cert-manager will manage in your namespace secretName: example-tls A few things to note here: behind the scenes, cert-manager works with the Certificate custom resource. when using ingress annotations and labels, cert-manager uses another internal controller to pick up the ingress resources and create a Certificate resource on your behalf. Of course, you can instead define this directly yourself but you will also have to define annotations on the ingress resource to specify which secret should be used for TLS termination. This is the recommended and safest approach when migrating from kube-cert-manager to cert-manager . apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: commonName: www.example.com dnsNames: - www.example.com issuerRef: kind: ClusterIssuer # @Note: we support letsencrypt-prod and letsencrypt-staging (use the latter to test your cert-manager related manifests) name: letsencrypt-prod secretName: example-tls You can review, get, list and describe the Certificate like any other kubernetes resource within your namespace. $ kubectl -n project get certificate NAME AGE example-tls 1d # you can also review the certificaterequests, orders and challenges via $ kubectl -n project get orders $ kubectl -n project get challenge Network Policies Please note that as part of the implementation of cert-manager v0.13.1, a GlobalNetworkPolicy object managing ingress traffic for http01 challenges has been deployed. This means that you no longer need to have a NetworkPolicy in your namespaces allowing ingress traffic from port 8089 to the ephemeral pods that cert-manager creates to handle the http01 challenge. As a developer I want to retrieve a certificate for a service behind the vpn, or simply wish to use the DNS validation # When a site is internal / behind the vpn, in order to handle the challenge you need to switch to using a DNS challenge. This is done by adding the following to your ingress resource: annotation cert-manager.io/enabled: \"true\" label cert-manager.io/solver: route53 Very Important : in order to successfully switch to a DNS challenge, please ensure you have contacted the ACP team before attempting this for the first time on your sub-domain as the correct permissions need to exist to permit cert-manager to add records to the domain. apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example-tls labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: mysite.example.com dnsNames: - example.com acme: config: - dns01: provider: route53 domains: - mysite.example.com - example.com Or via ingress you would use apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: # @Note: get cert-manager to manage this ingress cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls As a developer I want to use LetsEncrypt staging while configuring my cert-manager resources # You should use the staging version of LetsEncrypt in order to not be impacted by rate limits of the production version while setting up and testing the cert-manager annotations and labels you specify on your resources. By default, the production version of the LetsEncrypt ACME servers is used. To use the staging version, use the cert-manager.io/cluster-issuer annotation: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal # @Note: we are specifying which cluster issuer to use cert-manager.io/cluster-issuer: letsencrypt-staging labels: cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls Not specifying this annotation is equivalent to specifying cert-manager.io/cluster-issuer: letsencrypt-prod . Please note that the certificates issued by the staging version of LetsEncrypt are not signed and should not be used in production. As a developer I want to get a certificate for a server with a DNS name longer than 63 characters # A certificate's commonName is used to create a Certificate Signing Request and populate a field that is limited to 63 characters. In order to get a certificate for a server with a DNS name longer than 63 characters, you need to specify a common name of less than 63 characters and add the desired DNS name as an additional entry to dnsNames . For example, with an Ingress : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: cert-manager.io/solver: route53 spec: rules: - host: my-rather-long-winded-service-name.my-namespace.subdomain.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com secretName: example-tls Or with a Certificate : apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: svc-1.my-namespace.subdomain.example.com dnsNames: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com Getting a Kubernetes Robot Token # Users # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token. Project Admins (Creating a robot token) # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project -> Service -> Kube Robot Tokens). Getting a Kubernetes Token # Users # Log into the Platform Hub . Go to the Projects section and find your project. On the Overview & People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it. Project Admins (Creating a user token) # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token. Network Policies # By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: <your-namespace-here> spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on. Controlling Egress Traffic # Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples Run Performance Tests on a service hosted on ACP # As a Service, I should: # Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time Assessed tools summary: # An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana. Pod Security Policies # By default all user deployments will inherit a default PodSecurityPolicy applied accross our Kubernetes clusters, which define a set of conditions that a pod must be configured with in order to run successfully. runAsUser # This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user <username>, cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this: Dockerfile # Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer=\"rohith.jayawardene@digital.homeoffice.gov.uk\" RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5 Deployment Spec # In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: \"{{ .IMAGE_NAME }}\" image: \"{{ .IMAGE }}:{{ .VERSION }}\" ... Using artifactory as a private npm registry # A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN Setting up a local environment # Get your username and API key from artifactory Visit https://artifactory.digital.homeoffice.gov.uk/artifactory/webapp/#/profile, make a note of your username, and if you don't already have an API key then generate one. base64 encode your API key echo -n <api key> | base64 Set local environment variables Copy your encoded password, and set the following environment variables in your bash profile: export NPM_AUTH_USERNAME=<username> export NPM_AUTH_TOKEN=<base64 encoded api key> You might then need to source your profile to load these environment variables. Setting up CI in drone # Request a bot token for artifactory You can do this through the ACP Support Portal . One of the ACP team will create a token and send it to you as an encrypted gpg file via email. Decrypt the token gpg --decrypt ./path/to/file.gpg Add the token to drone as a secret First, base64 encode the token: echo -n \"<token>\" | base64 Then add this token to drone as a secret: drone secret add UKHomeOffice/<repo> NPM_AUTH_TOKEN <base64-encoded-token> --event pull_request Note: You will need to make sure the event types are lowercase. If an event is capitalised, it won't match the standard events inside of drone Note: you will need to make the secret available to pull request builds to be able to run npm commands in pull request steps Expose secret to build steps You will need to configure any steps which use npm to be able to access the secret. Do this by adding a secret property to those steps as follows: my_step: image: node:8 secrets: - npm_auth_token commands: - npm install - npm test Expose username to build steps In addition, you will need to add the username (as you provided when creating your token) as an environment variable. The easiest way to do this is as a \"matrix\" variable, which makes the username available to all steps without needing to configure them all individually. matrix: NPM_AUTH_USERNAME: - <username> Publishing modules to artifactory # It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name. Setting publish registry Add publishConfig to package.json. This ensures that the module can only ever be published to the private registry, and misconfiguration won't accidentally make it public \"publishConfig\": { \"registry\": \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/\" } Add auth settings In your project's .npmrc file (create one if it does not already exist) add the following lines: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:email=test@example.com The email address can be anything, it just needs to be set. Add publish step to drone Add the following step to your .drone.yml file to publish a new version whenever you release a tag. publish: image: node:8 secrets: - npm_auth_token commands: - npm publish when: event: tag Now, when you push new tags to github then drone should publish them to the artifactory npm registry automatically. Using modules from artifactory as dependencies # Configure your project to use artifactory In the project which is has private modules as dependencies, add the following line to .npmrc in the root of the project (create this file if it does not exist). @<namespace>:registry = https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ This will ensure that any module under that namespace will only ever install from artifactory, and never from the public registry If using multiple namespaces then add a line for each namespace. If the modules you are installing are not namespaced in artifactory, you can add the line with the namespace removed (i.e. registry = ... ) but this will have a negative impact on install speed. You should then add the following line to your project's .npmrc if they are not already there: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} You should now be able to install modules from artifactory into your local development environment. Installing dependencies in docker # If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} . Provisioned Volumes and Storage Classes # In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\" Backups for EBS # Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the Support Portal so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot . TLS Passthrough # There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic) Example steps # First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: \"127.0.0.1\" - name: PROXY_SERVICE_PORT value: \"8080\" - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: \"FALSE\" - name: NAXSI_USE_DEFAULT_RULES value: \"FALSE\" - name: PORT_IN_HOST_HEADER value: \"FALSE\" volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" kubernetes.io/ingress.class: \"nginx-external\" name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: / Writing Dockerfiles # Dockerfile best practice # We recommend familiarising yourself with Docker's excellent guidance on this topic. It is often easier to build from an existing base image. To find such base images that are maintained by Home Office colleagues, you can search the UKHomeOffice organisation on Github for repos starting with \u2018docker-\u2019 - e.g.: docker-java11-mvn If you want to use a base image in the UKHomeOffice organisation that does not appear to be regularly maintained, please get in touch via the ACP Service Desk and we will arrange write access to that repo. Please make sure that any base image that you maintain adheres to the best practices set out by Docker, and includes instructions to update all existing packages - e.g.: yum install -y curl && yum clean all && rpm --rebuilddb Home Office CentOS base image # If none of the technology specific images work for you, you can either build on top of them or build from the base Centos image .","title":"How To's"},{"location":"how-to-docs/index.html#how-tos","text":"This tree contains a collection of how-to guides for Developers.","title":"How To's"},{"location":"how-to-docs/index.html#create-an-artifactory-access-token","text":"Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please raise the relevant support request via the Support Portal . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u<username>:<api-key> -XPOST \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token\" -d \"username=<robot-username>\" -d \"scope=member-of-groups:<appropriate-groups>\" -d \"expires_in=0\" where <robot-username> is the name of the access token and <appropriate-groups> is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Create an Artifactory access token"},{"location":"how-to-docs/index.html#kubernetes-pod-autoscaling","text":"For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Kubernetes Pod Autoscaling"},{"location":"how-to-docs/index.html#chisel","text":"The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination.","title":"Chisel"},{"location":"how-to-docs/index.html#a-working-example","text":"We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:v1.3.1 # Both Chisel Client & Server versions must match securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point.","title":"A Working Example"},{"location":"how-to-docs/index.html#debug-issues-with-your-deployments","text":"","title":"Debug Issues with your deployments"},{"location":"how-to-docs/index.html#debug-with-secrets","text":"Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set.","title":"Debug with secrets"},{"location":"how-to-docs/index.html#debugging-issues-with-your-deployments-to-the-platform","text":"If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application.","title":"Debugging issues with your deployments to the platform"},{"location":"how-to-docs/index.html#debugging-deployments","text":"We suggest the following steps:","title":"Debugging deployments"},{"location":"how-to-docs/index.html#1-check-your-deployment-replicaset-and-pods-created-properly","text":"$ kubectl get deployments $ kubectl get rs $ kubectl get pods","title":"1. Check your deployment, replicaset and pods created properly"},{"location":"how-to-docs/index.html#2-investigate-potential-issues-with-your-pods-this-is-most-likely","text":"If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.*","title":"2. Investigate potential issues with your pods (this is most likely)"},{"location":"how-to-docs/index.html#3-investigate-potential-issues-with-your-service","text":"A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name","title":"3. Investigate potential issues with your service"},{"location":"how-to-docs/index.html#4-investigate-potential-issues-with-ingress","text":"Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti <ingress-3879072234-5f4uq> -c <proxy> bash where <proxy> is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs <ingress-3879072234-5f4uq>","title":"4. Investigate potential issues with ingress"},{"location":"how-to-docs/index.html#dms-migration","text":"","title":"DMS Migration"},{"location":"how-to-docs/index.html#prerequisite","text":"The following need to be true before you follow this guide: * AWS console logon * Access to the DMS service from console * A region where STS has been activated","title":"Prerequisite"},{"location":"how-to-docs/index.html#dms-setup","text":"Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button.","title":"DMS Setup"},{"location":"how-to-docs/index.html#downscaling-services-out-of-hours","text":"In an effort to reduce costs on running the platform, we've enabled the capability to scale down specific resources Out Of Hours (OOH) for Non-Production and Production environments.","title":"Downscaling Services Out Of Hours"},{"location":"how-to-docs/index.html#aws-rds-relational-database-service","text":"RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Support Portal support request template . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage.","title":"AWS RDS (Relational Database Service)"},{"location":"how-to-docs/index.html#kubernetes-pods","text":"Automatically scale down Kubernetes Deployments & Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments & Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters.","title":"Kubernetes Pods"},{"location":"how-to-docs/index.html#drone-how-to","text":"","title":"Drone How To"},{"location":"how-to-docs/index.html#install-drone-cli","text":"Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN=<your_drone_token> If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly.","title":"Install Drone CLI"},{"location":"how-to-docs/index.html#activate-your-pipeline","text":"Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Sync your repository access rights with Drone by clicking the icon in the top right corner again with the Synchronize button - this needs to be applied everytime when a new repository is created. Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo.","title":"Activate your pipeline"},{"location":"how-to-docs/index.html#configure-your-pipeline","text":"In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name <...> with the name of your app. You should be able to watch your build succeed in the Drone UI.","title":"Configure your pipeline"},{"location":"how-to-docs/index.html#publishing-docker-images","text":"","title":"Publishing Docker images"},{"location":"how-to-docs/index.html#deployments","text":"","title":"Deployments"},{"location":"how-to-docs/index.html#using-another-repo","text":"It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/<deployment_repo> branch: <master> deploy_to: <uat> params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/<your_repo> 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs.","title":"Using Another Repo"},{"location":"how-to-docs/index.html#versioned-deployments","text":"When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update && apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment","title":"Versioned deployments"},{"location":"how-to-docs/index.html#migrating-your-pipeline","text":"","title":"Migrating your pipeline"},{"location":"how-to-docs/index.html#docker-in-docker","text":"The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push","title":"Docker-in-Docker"},{"location":"how-to-docs/index.html#services","text":"If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306","title":"Services"},{"location":"how-to-docs/index.html#variable-escaping","text":"Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} --> $${DOCKER_PASSWORD} ${DRONE_TAG} --> $${DRONE_TAG} ${DRONE_COMMIT_SHA} --> $${DRONE_COMMIT_SHA}","title":"Variable Escaping"},{"location":"how-to-docs/index.html#scanning-images-in-drone","text":"ACP provides Anchore as a scanning solution for images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well as any public images. Example pipeline: pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables) image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # Indicates the image is locally available local_image: true # This indicates we are willing tolerate any vulnerabilities which are below medium (valid values: negligible, low, medium, high, critical) tolerate: medium # An optional whitelist (comma separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: <PATH> # Indicates we should show all vulnerabilities regardless show_all_vulnerabilities: false # By default the plugin will exit will fail if any vulnerabilities are discovered which are not tolerated, # you change this behaviour by setting the below fail_on_detection: false","title":"Scanning Images in Drone"},{"location":"how-to-docs/index.html#qas","text":"","title":"Q&amp;As"},{"location":"how-to-docs/index.html#aws-ecr-for-private-docker-images","text":"AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials.","title":"AWS ECR for Private Docker Images"},{"location":"how-to-docs/index.html#creating-a-docker-repository","text":"Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates.","title":"Creating a Docker Repository"},{"location":"how-to-docs/index.html#generating-access-credentials","text":"Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it.","title":"Generating Access Credentials"},{"location":"how-to-docs/index.html#accessing-a-docker-repository","text":"Accessing the AWS Container Registry to Pull & Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated.","title":"Accessing a Docker Repository"},{"location":"how-to-docs/index.html#pre-requisites","text":"To follow the below steps you must have: * AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows * Docker (version 17.06 or above, check with docker --version )","title":"Pre-Requisites"},{"location":"how-to-docs/index.html#step-1-retrieve-an-authorisation-token","text":"Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com","title":"Step 1: Retrieve an authorisation token"},{"location":"how-to-docs/index.html#step-2-login-with-authorisation-token","text":"Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Steps 1 and 2 will also not work if you are using AWS CLI (version 2.*). Instead use $ aws_account_id=\"340268328991\" $ aws_region=\"eu-west-2\" $ ecr_url=\"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" $ aws --region \"${aws_region}\" ecr get-login-password \\ | docker login \\ --password-stdin \\ --username AWS \\ \"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" Login Succeeded","title":"Step 2: Login with Authorisation Token"},{"location":"how-to-docs/index.html#pulling-pushing-images","text":"Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f","title":"Pulling &amp; Pushing Images"},{"location":"how-to-docs/index.html#listing-images-housekeeping","text":"Using the AWS CLI you can list all images that have been pushed to a given repository which you have access to. For example: $ aws ecr list-images --repository-name acp/hello-world-app { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" }, { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ] } To delete old images, you must have write access enabled or perform the action via a Robot Account. Images can be deleted based on a provided tag or digest. When providing a digest, all image tags with the same digest are deleted together. $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageTag=v0.0.1 { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" } ], \"failures\": [] } $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageDigest=sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ], \"failures\": [] }","title":"Listing Images &amp; Housekeeping"},{"location":"how-to-docs/index.html#managing-image-deployments-via-drone-ci","text":"The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI.","title":"Managing Image Deployments via Drone CI"},{"location":"how-to-docs/index.html#using-ingress","text":"An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: \"nginx-internal\" # Indicate the ingress SHOULD speak TLS between itself and pods (best-practice) ingress.kubernetes.io/backend-protocol: \"HTTPS\" name: myapp-server-internal spec: rules: - host: \"myapp.myproject.homeoffice.gov.uk\" http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - \"myapp.myproject.homeoffice.gov.uk\" # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Please view the official documentation for a full list of available ingress-nginx annotations . Note: Where the prefix for the annotation in the docs references nginx.ingress.kubernetes.io/ , this should be changed to ingress.kubernetes.io/ when running within ACP (as per the above example).","title":"Using Ingress"},{"location":"how-to-docs/index.html#cert-manager","text":"","title":"Cert Manager"},{"location":"how-to-docs/index.html#very-important-upgrade-information","text":"cert-manager is being upgraded from v0.8 to v0.13.1. If you have cert-manager resources deployed in your namespaces, you MUST follow the instructions to upgrade from v0.8 to upgrade annotations and labels in order for them to be managed by the new version of cert-manager. To find out if you are using v0.8 cert-manager resources in your namespace, you can run: kubectl get certificates.certmanager.k8s.io Also LetsEncrypt will no longer be supporting PSG's kube-cert-manager from June 2020. So if you are using PSG kube-cert-manager to obtain certificates for your ingresses, you also need to migrate to JetStack's cert-manager v0.13.1 and follow the instructions to upgrade from PGS's kube-cert-manager To find out if you are using PSG kube-cert-manager to manage your ingresses certificates, you can run: kubectl get ingresses -o yaml | grep stable.k8s.psg.io Please also be aware that admission policies have been updated and will reject Ingress resources with annotations or labels supported by more than one certificate manager. There are currently ingresses with both cert-manager v0.8 and PSG annotations or labels and those will now fail applying.","title":"VERY IMPORTANT upgrade information"},{"location":"how-to-docs/index.html#background","text":"The ACP platform presently has two certificate management services. The first service was PSG's kube-cert-manager . However with the forever changing landscape the project gradually became deprecated and now recommends replacement with JetStack's cert-manager . Therefore, projects still using kube-cert-manager should modify their services to start using cert-manager instead. Note that ACP will continue to support kube-cert-manager and the internal cfssl service while they are still in use, but we do recommend shifting over to cert-manager as soon as possible as aside from security fixes there will not be any more updates to these services. Without wishing to duplicate documentation which can be found in the readme and or official documentation , cert-manager can effectively replace two services: kube-cert-manager: used to acquire certificates from LetsEncrypt. cfssl: an internal Cloudflare service used to generate internal certificate (usually to encrypt between ingress and pod) . IMPORTANT NOTE: cert-manager is being upgraded from v0.8 to v0.13.1. In order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io","title":"Background"},{"location":"how-to-docs/index.html#how-tos_1","text":"","title":"How-tos"},{"location":"how-to-docs/index.html#as-a-developer-i-already-have-a-certificate-from-the-legacy-kube-cert-manager-how-do-i-migrate","text":"Migrating from the former kube-cert-manager over to cert-manager means creating the certificate request as below and removing the annotations from the ingress. However, the safe way would be to; Create a new Certificate resource and point to a new secret name (thus keeping the old one incase) . Push out the change and wait for the certificate to be fulfilled. Once you have the certificate you can update your ingress to use the new secret, remove the annotations and use the Certificate resource thereafter.","title":"As a developer I already have a certificate from the legacy kube-cert-manager, how do I migrate?"},{"location":"how-to-docs/index.html#as-a-developer-i-want-to-retrieve-an-internal-certificate","text":"As stated above the cert-manager can also handle internal certificates i.e. those signed by the internal ACP Certificate Authority (this is self signed btw) . At the moment you might be using cfssl-sidekick to perform this, but this can be completely replaced. If you want to create a certificate for a service, assuming the service is called myservice in namespace mynamespace , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Ingress resources are checked by admission policies to ensure the platform-ca cluster issuer only issues certificates for DNS names that are hosted inside the namespace. So if you want to create a certificate for a replica in a statefulset, assuming your statufelset is called mysts , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - mysts-0.myservice.mynamespace - mysts-0.myservice.mynamespace.svc - mysts-0.myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Note that mysts-0.myservice is intentionally missing from the list in dnsNames because those names need to be either a hostname (for the service) or a name ending with mynamespace , mynamespace.svc or mynamespace.svc.cluster.local . This would create a kubernetes secret named tls in your namespace with the signed certificate. An interesting thing to note here is that although this is using the ClusterIssuer platform-ca created by the ACP team, there is nothing stopping a project from creating a local Issuer for their own project. So for example. --- apiVersion: cert-manager.io/v1alpha2 kind: Issuer metadata: name: project-ca spec: ca: secretName: project-ca --- apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: project-ca # @Note: we have change from ClusterIssuer to a local Issuer kind: Issuer commonName: site.svc.project.cluster.local dnsNames: - localhost ipAddresses: - 127.0.0.1 Finally, if you want to use your certificate for client auth (as well as server auth in the following example), you need to add a keyUsages section to your Certificate resource: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer keyUsages: - server auth - client auth dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1","title":"As a developer I want to retrieve an internal certificate"},{"location":"how-to-docs/index.html#as-a-developer-i-want-to-retrieve-a-certificate-for-my-external-service","text":"Let's assume we have an externally facing site which we wish to expose via ingress and we want a valid LetsEncrypt certificate. Getting a certificate associated with the external ingress only requires to annotate the ingress with cert-manager.io/enabled , which is a toggle to ask cert-manager to handle this ingress resource. Optionally, the acme solver to be used by the cluster issuer can be specified with label cert-manager.io/solver: http01 . However, this is not required as the http01 acme solver is the default one. Please note that cert-manager.io/enabled is an annotation but cert-manager.io/solver is a label. When the site is externally facing i.e. the ingress class on the ingress is kubernetes.io/ingress.class: nginx-external you should always default to using a http01 challenge. However, if you know that the domain whitelisted in your namespace is hosted in AWS by Route53, you can instead specify a label of cert-manager.io/solver: route53 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: # @NOTE: this will enable cert-manager to handle this resource cert-manager.io/enabled: \"true\" ingress.kubernetes.io/affinity: cookie ingress.kubernetes.io/force-ssl-redirect: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/session-cookie-name: ingress ingress.kubernetes.io/ssl-redirect: \"true\" kubernetes.io/ingress.class: nginx-external name: example # @NOTE: the following label can be specified to ask letsencrypt to use the http01 acme challenge # @NOTE: but it is not required as http01 is the default solver labels: cert-manager.io/solver: http01 spec: rules: - host: www.example.com http: paths: - backend: serviceName: service_name servicePort: 10443 path: / tls: - hosts: - www.example.com # @NOTE: this is the name of the kubernetes secret that cert-manager will manage in your namespace secretName: example-tls A few things to note here: behind the scenes, cert-manager works with the Certificate custom resource. when using ingress annotations and labels, cert-manager uses another internal controller to pick up the ingress resources and create a Certificate resource on your behalf. Of course, you can instead define this directly yourself but you will also have to define annotations on the ingress resource to specify which secret should be used for TLS termination. This is the recommended and safest approach when migrating from kube-cert-manager to cert-manager . apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: commonName: www.example.com dnsNames: - www.example.com issuerRef: kind: ClusterIssuer # @Note: we support letsencrypt-prod and letsencrypt-staging (use the latter to test your cert-manager related manifests) name: letsencrypt-prod secretName: example-tls You can review, get, list and describe the Certificate like any other kubernetes resource within your namespace. $ kubectl -n project get certificate NAME AGE example-tls 1d # you can also review the certificaterequests, orders and challenges via $ kubectl -n project get orders $ kubectl -n project get challenge Network Policies Please note that as part of the implementation of cert-manager v0.13.1, a GlobalNetworkPolicy object managing ingress traffic for http01 challenges has been deployed. This means that you no longer need to have a NetworkPolicy in your namespaces allowing ingress traffic from port 8089 to the ephemeral pods that cert-manager creates to handle the http01 challenge.","title":"As a developer I want to retrieve a certificate for my external service"},{"location":"how-to-docs/index.html#as-a-developer-i-want-to-retrieve-a-certificate-for-a-service-behind-the-vpn-or-simply-wish-to-use-the-dns-validation","text":"When a site is internal / behind the vpn, in order to handle the challenge you need to switch to using a DNS challenge. This is done by adding the following to your ingress resource: annotation cert-manager.io/enabled: \"true\" label cert-manager.io/solver: route53 Very Important : in order to successfully switch to a DNS challenge, please ensure you have contacted the ACP team before attempting this for the first time on your sub-domain as the correct permissions need to exist to permit cert-manager to add records to the domain. apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example-tls labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: mysite.example.com dnsNames: - example.com acme: config: - dns01: provider: route53 domains: - mysite.example.com - example.com Or via ingress you would use apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: # @Note: get cert-manager to manage this ingress cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls","title":"As a developer I want to retrieve a certificate for a service behind the vpn, or simply wish to use the DNS validation"},{"location":"how-to-docs/index.html#as-a-developer-i-want-to-use-letsencrypt-staging-while-configuring-my-cert-manager-resources","text":"You should use the staging version of LetsEncrypt in order to not be impacted by rate limits of the production version while setting up and testing the cert-manager annotations and labels you specify on your resources. By default, the production version of the LetsEncrypt ACME servers is used. To use the staging version, use the cert-manager.io/cluster-issuer annotation: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal # @Note: we are specifying which cluster issuer to use cert-manager.io/cluster-issuer: letsencrypt-staging labels: cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls Not specifying this annotation is equivalent to specifying cert-manager.io/cluster-issuer: letsencrypt-prod . Please note that the certificates issued by the staging version of LetsEncrypt are not signed and should not be used in production.","title":"As a developer I want to use LetsEncrypt staging while configuring my cert-manager resources"},{"location":"how-to-docs/index.html#as-a-developer-i-want-to-get-a-certificate-for-a-server-with-a-dns-name-longer-than-63-characters","text":"A certificate's commonName is used to create a Certificate Signing Request and populate a field that is limited to 63 characters. In order to get a certificate for a server with a DNS name longer than 63 characters, you need to specify a common name of less than 63 characters and add the desired DNS name as an additional entry to dnsNames . For example, with an Ingress : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: cert-manager.io/solver: route53 spec: rules: - host: my-rather-long-winded-service-name.my-namespace.subdomain.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com secretName: example-tls Or with a Certificate : apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: svc-1.my-namespace.subdomain.example.com dnsNames: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com","title":"As a developer I want to get a certificate for a server with a DNS name longer than 63 characters"},{"location":"how-to-docs/index.html#getting-a-kubernetes-robot-token","text":"","title":"Getting a Kubernetes Robot Token"},{"location":"how-to-docs/index.html#users","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token.","title":"Users"},{"location":"how-to-docs/index.html#project-admins-creating-a-robot-token","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project -> Service -> Kube Robot Tokens).","title":"Project Admins (Creating a robot token)"},{"location":"how-to-docs/index.html#getting-a-kubernetes-token","text":"","title":"Getting a Kubernetes Token"},{"location":"how-to-docs/index.html#users_1","text":"Log into the Platform Hub . Go to the Projects section and find your project. On the Overview & People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it.","title":"Users"},{"location":"how-to-docs/index.html#project-admins-creating-a-user-token","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token.","title":"Project Admins (Creating a user token)"},{"location":"how-to-docs/index.html#network-policies","text":"By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: <your-namespace-here> spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on.","title":"Network Policies"},{"location":"how-to-docs/index.html#controlling-egress-traffic","text":"Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples","title":"Controlling Egress Traffic"},{"location":"how-to-docs/index.html#run-performance-tests-on-a-service-hosted-on-acp","text":"","title":"Run Performance Tests on a service hosted on ACP"},{"location":"how-to-docs/index.html#as-a-service-i-should","text":"Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time","title":"As a Service, I should:"},{"location":"how-to-docs/index.html#assessed-tools-summary","text":"An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana.","title":"Assessed tools summary:"},{"location":"how-to-docs/index.html#pod-security-policies","text":"By default all user deployments will inherit a default PodSecurityPolicy applied accross our Kubernetes clusters, which define a set of conditions that a pod must be configured with in order to run successfully.","title":"Pod Security Policies"},{"location":"how-to-docs/index.html#runasuser","text":"This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user <username>, cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this:","title":"runAsUser"},{"location":"how-to-docs/index.html#dockerfile","text":"Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer=\"rohith.jayawardene@digital.homeoffice.gov.uk\" RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5","title":"Dockerfile"},{"location":"how-to-docs/index.html#deployment-spec","text":"In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: \"{{ .IMAGE_NAME }}\" image: \"{{ .IMAGE }}:{{ .VERSION }}\" ...","title":"Deployment Spec"},{"location":"how-to-docs/index.html#using-artifactory-as-a-private-npm-registry","text":"A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN","title":"Using artifactory as a private npm registry"},{"location":"how-to-docs/index.html#setting-up-a-local-environment","text":"","title":"Setting up a local environment"},{"location":"how-to-docs/index.html#setting-up-ci-in-drone","text":"","title":"Setting up CI in drone"},{"location":"how-to-docs/index.html#publishing-modules-to-artifactory","text":"It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name.","title":"Publishing modules to artifactory"},{"location":"how-to-docs/index.html#using-modules-from-artifactory-as-dependencies","text":"","title":"Using modules from artifactory as dependencies"},{"location":"how-to-docs/index.html#installing-dependencies-in-docker","text":"If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} .","title":"Installing dependencies in docker"},{"location":"how-to-docs/index.html#provisioned-volumes-and-storage-classes","text":"In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\"","title":"Provisioned Volumes and Storage Classes"},{"location":"how-to-docs/index.html#backups-for-ebs","text":"Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the Support Portal so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot .","title":"Backups for EBS"},{"location":"how-to-docs/index.html#tls-passthrough","text":"There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic)","title":"TLS Passthrough"},{"location":"how-to-docs/index.html#example-steps","text":"First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: \"127.0.0.1\" - name: PROXY_SERVICE_PORT value: \"8080\" - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: \"FALSE\" - name: NAXSI_USE_DEFAULT_RULES value: \"FALSE\" - name: PORT_IN_HOST_HEADER value: \"FALSE\" volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" kubernetes.io/ingress.class: \"nginx-external\" name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: /","title":"Example steps"},{"location":"how-to-docs/index.html#writing-dockerfiles","text":"","title":"Writing Dockerfiles"},{"location":"how-to-docs/index.html#dockerfile-best-practice","text":"We recommend familiarising yourself with Docker's excellent guidance on this topic. It is often easier to build from an existing base image. To find such base images that are maintained by Home Office colleagues, you can search the UKHomeOffice organisation on Github for repos starting with \u2018docker-\u2019 - e.g.: docker-java11-mvn If you want to use a base image in the UKHomeOffice organisation that does not appear to be regularly maintained, please get in touch via the ACP Service Desk and we will arrange write access to that repo. Please make sure that any base image that you maintain adheres to the best practices set out by Docker, and includes instructions to update all existing packages - e.g.: yum install -y curl && yum clean all && rpm --rebuilddb","title":"Dockerfile best practice"},{"location":"how-to-docs/index.html#home-office-centos-base-image","text":"If none of the technology specific images work for you, you can either build on top of them or build from the base Centos image .","title":"Home Office CentOS base image"},{"location":"how-to-docs/artifactory-token.html","text":"Create an Artifactory access token # Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please raise the relevant support request via the Support Portal . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u<username>:<api-key> -XPOST \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token\" -d \"username=<robot-username>\" -d \"scope=member-of-groups:<appropriate-groups>\" -d \"expires_in=0\" where <robot-username> is the name of the access token and <appropriate-groups> is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Artifactory token"},{"location":"how-to-docs/artifactory-token.html#create-an-artifactory-access-token","text":"Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please raise the relevant support request via the Support Portal . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u<username>:<api-key> -XPOST \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token\" -d \"username=<robot-username>\" -d \"scope=member-of-groups:<appropriate-groups>\" -d \"expires_in=0\" where <robot-username> is the name of the access token and <appropriate-groups> is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Create an Artifactory access token"},{"location":"how-to-docs/autoscaling.html","text":"Kubernetes Pod Autoscaling # For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Autoscaling"},{"location":"how-to-docs/autoscaling.html#kubernetes-pod-autoscaling","text":"For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Kubernetes Pod Autoscaling"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html","text":"Migration from PSG kube-cert-manager resources to Jetstack cert-manager v0.13.1 # Table of Content # Background (understanding why a migration is needed) Migration options Option 1 (renaming secrets) Option 2 (explicit ingress certificate) Getting cert-manager resources Updating cert-manager resources for v0.13.1 Option 1 changes (recommended) External Ingress with DNS challenge changes (recommended) External Ingress with HTTP challenge changes (recommended) Internal Ingress with DNS challenge changes (recommended) Option 2 changes External Ingress with DNS challenge changes (2 stages) External Ingress with HTTP challenge changes (2 stages) Internal Ingress with DNS challenge changes (2 stages) Deployment verification Background (understanding why a migration is needed) # PalmStoneGames/kube-cert-manager has been deprecated and not updated for 2 years, but more importantly it will stop being supported by LetsEncrypt in June 2020. Migration options # There are 2 possible approaches for the migration of Ingress resources. The high-level steps for both approaches are expanded below. For a more detailed understanding of how the manifest files need to be updated, please refer to section Updating cert-manager resources for v0.13.1 below. Option 1 below is strongly recommended as the approach. Option 1 (renaming secrets) # By far the easiest and safest option is to amend the annotations and labels of your Ingress resources as described below while at the same time also renaming the associated secrets . Renaming the secret (changing the value of secretName in your Ingress resource) will make sure that the same secret is not managed by 2 certificate managers (PSG kube-cert-manager and JetStack's cert-manager v0.13.1). To keep names consistent, you could for example add a -cmio suffix (standing for cert-manager.io ) to all the secretName attributes in the Ingress resources as shown below in the example section. Amend Ingress resources: amend the Ingress 's annotations and labels as described below change the value of secretName Deploy the changes When you've checked that the service is functioning as intended, you can tidy up the old secrets that used to be managed by PSG kube-cert-manager: delete any secrets that was previously associated with the Ingress (back them up to be safe) You can check that the certificate resources automatically created by cert-manager thanks to your ingress annotations are valid by running kubectl -n project get certificate.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. The main draw-back of this approach is that the value for the new secret being created will need to be created from LetsEncrypt. This is usually quite quick, but could take up to around 2 minutes. During the time the new certificate is being requested and LetsEncrypt performs its http or dns challenge, your ingress will not have a valid certificate. So access to the endpoint is disrupted for that period whilst the challenge is being completed and new cert/secret generated. If you are keen on minimising service disruption further and only have current connections reset, please evaluate Option 2 below. Option 2 (explicit ingress certificate) # This option is more complex than Option 1 and should only be considered if there are concerens with service availability while ingresses do not have a valid certifcate during the initial new certificate request. If not performed properly, you will gain nothing from it and it will have the same impact as Option 1. The high levels steps are: Leave the current Ingress resource as it is (whith the PSG annotations) Create a Certificate resource with letsencrypt-prod as the clusterIssuer, a secret name different from the one used by the Ingress and the appropriate stanzas as shown later on this guide. You might want to use the letsencrypt-staging clusterIssuer instead of letsencrypt-prod while changing your Certificate manifest file and testing it in order to not reach the weekly limits imposed by LetsEncrypt on its prod server and switch to letsencrypt-prod once you know your Certificate resource works as you expect. Deploy the changes to create the new Certificate resource. Please note that the certificate and associated secret will at that point be unused, but make sure the Certificate is ready before deploying the next set of changes. You can check that by running kubectl get certificates.cert-manager.io in your namespace. Update your Ingress resources Remove all stable.k8s.psg.io annotations and labels DO NOT add any new cert-manager.io annotations or labels Update secretName in the Ingress resource to the name of the secret you created in step 2 (the secret associated with your Certficate resource) Deploy the Ingress changes When you've checked that the service is functioning as intended and that its certificate has been updated, you can tidy up the old secrets: delete any secrets previously associated with the ingress (back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificates.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. Please note that during the development lifecycle, you will quite naturally deploy the 2 changes above when they are made in 2 separate commit points. However, when it comes to deploying to other environments once the commits already exist, make sure to deploy the commit associated with the first step ( Certificate creation), wait for the Certificate resource to be ready and only deploy the change to the ingress once it is. If you do not wait and deploy those changes in quick succession, the outcome will be the same as for option 1: your service will be unavailable as it will not have a valid certificate until letsencrypt returns a new one. Getting cert-manager resources # As cert-manager is being upgraded from v0.8 to v0.13.1 and in order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io Updating cert-manager resources for v0.13.1 # The following examples are based on the kube-example project. Option 1 changes (recommended) # External Ingress with DNS challenge changes (recommended) # Changes required for websites or services exposed externally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio External Ingress with HTTP challenge changes (recommended) # Changes required for websites or services exposed externally with ACME HTTP challenge The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" stable.k8s.psg.io/kcm.provider: http labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the http01 solver should be used # @Note: the label below is actually optional because http01 the default value cert-manager.io/solver: http01 spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio Internal Ingress with DNS challenge changes (recommended) # Changes required for websites or services exposed internally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any psg annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio Option 2 changes # External Ingress with DNS challenge changes (2 stages) # Changes required for websites or services exposed externally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio External Ingress with HTTP challenge changes (2 stages) # Changes required for websites or services exposed externally The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" stable.k8s.psg.io/kcm.provider: http labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the http01 solver should be used # @Note: alternatively, specify no cert-manager.io/solver label as http01 is the default cert-manager.io/solver: http01 spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio Internal Ingress with DNS challenge changes (2 stages) # Changes required for websites or services exposed internally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_INTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio Deployment verification # Once you have applied the changes above, you should no longer have Ingress resources managed by PSG kube-cert-manager. To verify that's the case, run kubectl get ingresses -o yaml | grep stable.k8s.psg.io . An empty list should be returned. To verify that the new version of cert-manager is managing the certificates, run kubectl get certificate.cert-manager.io -n my-namespace . An non-empty list of resources should be returned. Those are the Certificate resources which have been created by cert-manager's ingress shim. Once the development teams have migrated all their resources to the new version of cert-manager, the PSG kube-cert-manager and cert-manager v0.8 will be decommissioned. When that's done it will no longer be needed to append .cert-manager.io to the resource kinds when using kubectl .","title":"Migration from PSG kube-cert-manager resources to Jetstack cert-manager v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#migration-from-psg-kube-cert-manager-resources-to-jetstack-cert-manager-v0131","text":"","title":"Migration from PSG kube-cert-manager resources to Jetstack cert-manager v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#table-of-content","text":"Background (understanding why a migration is needed) Migration options Option 1 (renaming secrets) Option 2 (explicit ingress certificate) Getting cert-manager resources Updating cert-manager resources for v0.13.1 Option 1 changes (recommended) External Ingress with DNS challenge changes (recommended) External Ingress with HTTP challenge changes (recommended) Internal Ingress with DNS challenge changes (recommended) Option 2 changes External Ingress with DNS challenge changes (2 stages) External Ingress with HTTP challenge changes (2 stages) Internal Ingress with DNS challenge changes (2 stages) Deployment verification","title":"Table of Content"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#background-understanding-why-a-migration-is-needed","text":"PalmStoneGames/kube-cert-manager has been deprecated and not updated for 2 years, but more importantly it will stop being supported by LetsEncrypt in June 2020.","title":"Background (understanding why a migration is needed)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#migration-options","text":"There are 2 possible approaches for the migration of Ingress resources. The high-level steps for both approaches are expanded below. For a more detailed understanding of how the manifest files need to be updated, please refer to section Updating cert-manager resources for v0.13.1 below. Option 1 below is strongly recommended as the approach.","title":"Migration options"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#option-1-renaming-secrets","text":"By far the easiest and safest option is to amend the annotations and labels of your Ingress resources as described below while at the same time also renaming the associated secrets . Renaming the secret (changing the value of secretName in your Ingress resource) will make sure that the same secret is not managed by 2 certificate managers (PSG kube-cert-manager and JetStack's cert-manager v0.13.1). To keep names consistent, you could for example add a -cmio suffix (standing for cert-manager.io ) to all the secretName attributes in the Ingress resources as shown below in the example section. Amend Ingress resources: amend the Ingress 's annotations and labels as described below change the value of secretName Deploy the changes When you've checked that the service is functioning as intended, you can tidy up the old secrets that used to be managed by PSG kube-cert-manager: delete any secrets that was previously associated with the Ingress (back them up to be safe) You can check that the certificate resources automatically created by cert-manager thanks to your ingress annotations are valid by running kubectl -n project get certificate.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. The main draw-back of this approach is that the value for the new secret being created will need to be created from LetsEncrypt. This is usually quite quick, but could take up to around 2 minutes. During the time the new certificate is being requested and LetsEncrypt performs its http or dns challenge, your ingress will not have a valid certificate. So access to the endpoint is disrupted for that period whilst the challenge is being completed and new cert/secret generated. If you are keen on minimising service disruption further and only have current connections reset, please evaluate Option 2 below.","title":"Option 1 (renaming secrets)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#option-2-explicit-ingress-certificate","text":"This option is more complex than Option 1 and should only be considered if there are concerens with service availability while ingresses do not have a valid certifcate during the initial new certificate request. If not performed properly, you will gain nothing from it and it will have the same impact as Option 1. The high levels steps are: Leave the current Ingress resource as it is (whith the PSG annotations) Create a Certificate resource with letsencrypt-prod as the clusterIssuer, a secret name different from the one used by the Ingress and the appropriate stanzas as shown later on this guide. You might want to use the letsencrypt-staging clusterIssuer instead of letsencrypt-prod while changing your Certificate manifest file and testing it in order to not reach the weekly limits imposed by LetsEncrypt on its prod server and switch to letsencrypt-prod once you know your Certificate resource works as you expect. Deploy the changes to create the new Certificate resource. Please note that the certificate and associated secret will at that point be unused, but make sure the Certificate is ready before deploying the next set of changes. You can check that by running kubectl get certificates.cert-manager.io in your namespace. Update your Ingress resources Remove all stable.k8s.psg.io annotations and labels DO NOT add any new cert-manager.io annotations or labels Update secretName in the Ingress resource to the name of the secret you created in step 2 (the secret associated with your Certficate resource) Deploy the Ingress changes When you've checked that the service is functioning as intended and that its certificate has been updated, you can tidy up the old secrets: delete any secrets previously associated with the ingress (back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificates.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. Please note that during the development lifecycle, you will quite naturally deploy the 2 changes above when they are made in 2 separate commit points. However, when it comes to deploying to other environments once the commits already exist, make sure to deploy the commit associated with the first step ( Certificate creation), wait for the Certificate resource to be ready and only deploy the change to the ingress once it is. If you do not wait and deploy those changes in quick succession, the outcome will be the same as for option 1: your service will be unavailable as it will not have a valid certificate until letsencrypt returns a new one.","title":"Option 2 (explicit ingress certificate)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#getting-cert-manager-resources","text":"As cert-manager is being upgraded from v0.8 to v0.13.1 and in order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io","title":"Getting cert-manager resources"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#updating-cert-manager-resources-for-v0131","text":"The following examples are based on the kube-example project.","title":"Updating cert-manager resources for v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#option-1-changes-recommended","text":"","title":"Option 1 changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#external-ingress-with-dns-challenge-changes-recommended","text":"Changes required for websites or services exposed externally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress with DNS challenge changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#external-ingress-with-http-challenge-changes-recommended","text":"Changes required for websites or services exposed externally with ACME HTTP challenge The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" stable.k8s.psg.io/kcm.provider: http labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the http01 solver should be used # @Note: the label below is actually optional because http01 the default value cert-manager.io/solver: http01 spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress with HTTP challenge changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#internal-ingress-with-dns-challenge-changes-recommended","text":"Changes required for websites or services exposed internally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG labels and annotations apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any psg annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: add the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" labels: # @Note: remove any psg labels you might have # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio","title":"Internal Ingress with DNS challenge changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#option-2-changes","text":"","title":"Option 2 changes"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#external-ingress-with-dns-challenge-changes-2-stages","text":"Changes required for websites or services exposed externally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress with DNS challenge changes (2 stages)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#external-ingress-with-http-challenge-changes-2-stages","text":"Changes required for websites or services exposed externally The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" stable.k8s.psg.io/kcm.provider: http labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the http01 solver should be used # @Note: alternatively, specify no cert-manager.io/solver label as http01 is the default cert-manager.io/solver: http01 spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress with HTTP challenge changes (2 stages)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#internal-ingress-with-dns-challenge-changes-2-stages","text":"Changes required for websites or services exposed internally with ACME DNS challenge suitable when your domain is hosted as a Route53 zone. The following Ingress resource with PSG annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: your ingress might not specify stable.k8s.psg.io/kcm.provider as dns is the default provider stable.k8s.psg.io/kcm.provider: dns labels: stable.k8s.psg.io/kcm.class: default spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio labels: # @Note: specify label cert-manager.io/solver to specify that the route53 dns01 solver should be used cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_INTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any psg annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio","title":"Internal Ingress with DNS challenge changes (2 stages)"},{"location":"how-to-docs/cert-manager-upgrade-from-psg.html#deployment-verification","text":"Once you have applied the changes above, you should no longer have Ingress resources managed by PSG kube-cert-manager. To verify that's the case, run kubectl get ingresses -o yaml | grep stable.k8s.psg.io . An empty list should be returned. To verify that the new version of cert-manager is managing the certificates, run kubectl get certificate.cert-manager.io -n my-namespace . An non-empty list of resources should be returned. Those are the Certificate resources which have been created by cert-manager's ingress shim. Once the development teams have migrated all their resources to the new version of cert-manager, the PSG kube-cert-manager and cert-manager v0.8 will be decommissioned. When that's done it will no longer be needed to append .cert-manager.io to the resource kinds when using kubectl .","title":"Deployment verification"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html","text":"Migration from Jetstack cert-manager v0.8 resources to v0.13.1 # Table of Content # Background (understanding why a migration is needed) Migration options Option 1 (renaming secrets) Option 2 (explicit ingress certificate) Getting cert-manager resources Updating cert-manager resources for v0.13.1 Option 1 changes (recommended) External Ingress changes (recommended) Internal Ingress changes (recommended) Option 2 changes External Ingress changes (2 stages) Internal Ingress changes (2 stages) Certificate resources changes Network Policy resources changes Deployment verification There is a lot of information in this migration guide, so please make sure you read it all and understand what is required before performing a migration, as it might have an adverse impact on services if not performed appropriately. Background (understanding why a migration is needed) # In version v0.11, cert-manager introduced some backwards incompatible changes that were announced in v0.8. Because there are currently 2 instances of cert-manager running on the ACP platform, it is important that your cert-manager related resources are only managed by one of those instances. Otherwise, you run the risk of hitting letsencrypt rate limits as the 2 instances of cert-manager both attempt to manage the same resources. See the Migration Options section below to determine which one is most appropriate to you The following official cert-manager documentation provides good background information as to what has changed. v0.8 Release Notes v0.11 Release Notes Upgrading from v0.7 to v0.8 Upgrading from v0.10 to v0.11 Essentially, the cert-manager API version has been changed from certmanager.k8s.io/v1alpha1 to cert-manager.io/v1alpha2 . This means that both Ingress and Certificate resources have to be changed: there are annotations and labels changes for both resource types, as well as structural changes for Certificate s. Please note that when updating annotations and labels, you should only do that on resources you have created yourselves (not the ones managed automatically by cert-manager). For example, if you your manifest file contains a Certificate resource definition to terminate TLS on a sidecar, you should update annotations and labels as described below. However, please be aware that when annotating an Ingress resource with cert-manager annotations, cert-manager will automatically create a Certificate resource to handle certificate resources to LetsEncrypt. Those Certificate resources, which have the same name as the secret name specified in the Ingress , are internally managed by cert-manager should not be modified. Migration options # There are 2 possible approaches for the migration of resources. The high-level steps for both approaches are expanded below. For a more detailed understanding of how the manifest files need to be updated, please refer to section Updating cert-manager resources for v0.13.1 below. Option 1 below is strongly recommended as the approach. Option 1 (renaming secrets) # By far the easiest and safest option is to amend the annotations and labels as described below while at the same time also renaming the associated secrets . Renaming the secret (changing the value of secretName in either Ingress or Certificate resources) will make sure that the same secret is not managed by 2 Certificate resources (the v0.8 certificate and its v0.13.1 counter-part) - whether those Certificate resources are part of your deployments or it's one of the resources managed automatically for you by cert-manager when it deals with Ingress annotations. To keep names consistent, you could for example add a -cmio suffix (standing for cert-manager.io ) to all the secretName attributes in the Ingress or Certificate resources as shown below in the example section. For Ingress resources: amend the Ingress 's annotations and labels as described below change the value of secretName For Certificate resources that are part of your deployments (e.g. to create a certificate that is mounted by an nginx sidecar for your main service): amend the Certificate 's annotations and labels change the value of secretName amend your deployment to mount the new secret (e.g. get the nginx sidecar to mount the new tls secret) Deploy the changes When you've checked that the service is functioning as intended, you can tidy up the old v0.8 cert-manager resources: delete any certificate resources still returned by kubectl -n project get certificate (back them up if they are not stored in git) delete any secrets associated with those old resources (again, back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificate.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. The main draw-back of this approach is that the value for the new secret being created will need to be created from LetsEncrypt (unless using the platform cluster issuer). This is usually quite quick, but could take up to around 2 minutes. During the time the new certificate is being requested and LetsEncrypt performs its http or dns challenge, your ingress will not have a valid certificate. So access to the endpoint is disrupted for that period whilst the challenge is being completed and new cert/secret generated. If you are keen on minimising service disruption further and only have current connections reset, please evaluate Option 2 below. Option 2 (explicit ingress certificate) # This option is more complex than Option 1 and should only be considered if there are concerns with service availability while ingresses do not have a valid certifcate during the initial new certificate request. If not performed properly, you will gain nothing from it and it will have the same impact as Option 1. The high levels steps are: Leave the current Ingress resource as it is (whith old v0.8 annotations) Create a Certificate resource with letsencrypt-prod as the clusterIssuer, a secret name different from the one used by the Ingress and the appropriate stanzas as shown later on this guide. You might want to use the letsencrypt-staging clusterIssuer instead of letsencrypt-prod while changing your Certificate manifest file and testing it in order to not reach the weekly limits imposed by LetsEncrypt on its prod server and switch to letsencrypt-prod once you know your Certificate resource works as you expect. Deploy the changes to create the new certificate resource. Please note that the certificate and associated secret will at that point be unused, but make sure the Certificate is ready before deploying the next set of changes. You can check that by running kubectl get certificates.cert-manager.io in your namespace. Update your Ingress resources Remove all certmanager.k8s.io annotations DO NOT add any new cert-manager.io annotations or labels Update secretName in the Ingress resource to the name of the secret you created in step 2 (the secret associated with your Certficate resource) Deploy the Ingress changes When you've checked that the service is functioning as intended and that its certificate has been updated, you can tidy up the old v0.8 cert-manager resources and secrets: delete any certificate resources still returned by kubectl -n project get certificates.certmanager.k8s.io (back them up if they are not stored in git) delete any secrets associated with those old resources (again, back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificates.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. Please note that during the development lifecycle, you will quite naturally deploy the 2 changes above when they are made in 2 separate commit points. However, when it comes to deploying to other environments once the commits already exist, make sure to deploy the commit associated with the first step ( Certificate creation), wait for the Certificate resource to be ready and only deploy the change to the ingress once it is. If you do not wait and deploy those changes in quick succession, the outcome will be the same as for option 1: your service will be unavailable as it will not have a valid certificate until letsencrypt returns a new one. Getting cert-manager resources # As cert-manager is being upgraded from v0.8 to v0.13.1 and in order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io Updating cert-manager resources for v0.13.1 # The following examples are based on the kube-example project. Option 1 changes (recommended) # External Ingress changes (recommended) # Changes required for websites or services exposed externally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: certmanager.k8s.io/acme-challenge-type: \"http01\" certmanager.k8s.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: change the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio Internal Ingress changes (recommended) # Changes required for websites or services exposed internally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: certmanager.k8s.io/acme-challenge-type: \"dns01\" certmanager.k8s.io/enabled: \"true\" certmanager.k8s.io/acme-dns01-provider: \"route53\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: change the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used labels: cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio Option 2 changes # External Ingress changes (2 stages) # Changes required for websites or services exposed externally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: certmanager.k8s.io/acme-challenge-type: \"http01\" certmanager.k8s.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio Internal Ingress changes (2 stages) # Changes required for websites or services exposed internally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: certmanager.k8s.io/acme-challenge-type: \"dns01\" certmanager.k8s.io/enabled: \"true\" certmanager.k8s.io/acme-dns01-provider: \"route53\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio labels: cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_INTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: no cert-manager.io annotations or labels are added cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio Certificate resources changes # Please note that because the apiGroup for the new certificate resource ( cert-manager.io ) is different from the old one ( certmanager.k8s.io ), by making the changes below to your Certificate resource, you will actually create a new Certificate object as opposed to replacing the existing one. The following v0.8 Certificate resource providing a self-signed certificate: apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-service-tls spec: secretName: {{ .DEPLOYMENT_NAME }}-service-tls issuerRef: name: platform-tls kind: ClusterIssuer commonName: app.{{ .KUBE_NAMESPACE }}.svc.cluster.local dnsNames: - app - app.{{ .KUBE_NAMESPACE }}.svc should be changed to a v0.11 Certificate # @Note: change the apiVersion apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-service-tls spec: # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-service-tls-cmio issuerRef: # @Note: change the name of the issuer name: platform-ca kind: ClusterIssuer commonName: app.{{ .KUBE_NAMESPACE }}.svc.cluster.local dnsNames: - app - app.{{ .KUBE_NAMESPACE }}.svc In order to convert a Certificate resource for a certificate issued by LetsEncrypt, the spec.issuerRef.name should be set as letsencrypt-prod . For an externally accessed service, no labelling is required. However for an internally accessed service whose certificate ACME challenge is resolved with dns01, the following label should be added: cert-manager.io/solver: route53 . For internal certificates issued by the platform-ca cluster issuer, please be aware of the following: if you use your certificate for client auth, you need to add a keyUsages section to your certificate (please see the main cert-manager guide for more detail) if you want to create a certificate for replicas in your statefulset, you should specify a DNS name of mysts-0.myservice.mynamespace to avoid having your resource being rejected by the admission policies (please see the main cert-manager guide for more detail) Network Policy resources changes # The following NetworkPolicy resources used to be required to allow successful http01 ACME challenges for cert-manager v0.8 resources. This is no longer needed thanks to a new GlobalNetworkPolicy which has a cluster-wide scope. Network policies such as the following in your namespaces can therefore be deleted # No longer required; this NetworkPolicy can now be deleted kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: permit-certmanager-acme spec: policyTypes: - Ingress podSelector: matchExpressions: - {key: certmanager.k8s.io/acme-http-domain, operator: Exists} - {key: certmanager.k8s.io/acme-http-token, operator: Exists} ingress: - from: - namespaceSelector: matchLabels: name: ingress-external ports: - protocol: TCP port: 8089 Deployment verification # Once you have applied the changes above, you should no longer have Certificate resources managed by v0.8. To verify that's the case, run kubectl get certificate -n my-namespace . The resource list returned should be empty. To verify that the new version of cert-manager is managing the certificates, run kubectl get certificate.cert-manager.io -n my-namespace . An non-empty list of resources should be returned, including any certificate resources you have created yourselves as well as ones created by cert-manager on behalf of an ingress. Once the development teams have migrated all their resources to the new version of cert-manager, the old instance will be decommissioned. When that's done it will no longer be needed to append .cert-manager.io to the resource kinds when using kubectl .","title":"Migration from Jetstack cert-manager v0.8 resources to v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#migration-from-jetstack-cert-manager-v08-resources-to-v0131","text":"","title":"Migration from Jetstack cert-manager v0.8 resources to v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#table-of-content","text":"Background (understanding why a migration is needed) Migration options Option 1 (renaming secrets) Option 2 (explicit ingress certificate) Getting cert-manager resources Updating cert-manager resources for v0.13.1 Option 1 changes (recommended) External Ingress changes (recommended) Internal Ingress changes (recommended) Option 2 changes External Ingress changes (2 stages) Internal Ingress changes (2 stages) Certificate resources changes Network Policy resources changes Deployment verification There is a lot of information in this migration guide, so please make sure you read it all and understand what is required before performing a migration, as it might have an adverse impact on services if not performed appropriately.","title":"Table of Content"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#background-understanding-why-a-migration-is-needed","text":"In version v0.11, cert-manager introduced some backwards incompatible changes that were announced in v0.8. Because there are currently 2 instances of cert-manager running on the ACP platform, it is important that your cert-manager related resources are only managed by one of those instances. Otherwise, you run the risk of hitting letsencrypt rate limits as the 2 instances of cert-manager both attempt to manage the same resources. See the Migration Options section below to determine which one is most appropriate to you The following official cert-manager documentation provides good background information as to what has changed. v0.8 Release Notes v0.11 Release Notes Upgrading from v0.7 to v0.8 Upgrading from v0.10 to v0.11 Essentially, the cert-manager API version has been changed from certmanager.k8s.io/v1alpha1 to cert-manager.io/v1alpha2 . This means that both Ingress and Certificate resources have to be changed: there are annotations and labels changes for both resource types, as well as structural changes for Certificate s. Please note that when updating annotations and labels, you should only do that on resources you have created yourselves (not the ones managed automatically by cert-manager). For example, if you your manifest file contains a Certificate resource definition to terminate TLS on a sidecar, you should update annotations and labels as described below. However, please be aware that when annotating an Ingress resource with cert-manager annotations, cert-manager will automatically create a Certificate resource to handle certificate resources to LetsEncrypt. Those Certificate resources, which have the same name as the secret name specified in the Ingress , are internally managed by cert-manager should not be modified.","title":"Background (understanding why a migration is needed)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#migration-options","text":"There are 2 possible approaches for the migration of resources. The high-level steps for both approaches are expanded below. For a more detailed understanding of how the manifest files need to be updated, please refer to section Updating cert-manager resources for v0.13.1 below. Option 1 below is strongly recommended as the approach.","title":"Migration options"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#option-1-renaming-secrets","text":"By far the easiest and safest option is to amend the annotations and labels as described below while at the same time also renaming the associated secrets . Renaming the secret (changing the value of secretName in either Ingress or Certificate resources) will make sure that the same secret is not managed by 2 Certificate resources (the v0.8 certificate and its v0.13.1 counter-part) - whether those Certificate resources are part of your deployments or it's one of the resources managed automatically for you by cert-manager when it deals with Ingress annotations. To keep names consistent, you could for example add a -cmio suffix (standing for cert-manager.io ) to all the secretName attributes in the Ingress or Certificate resources as shown below in the example section. For Ingress resources: amend the Ingress 's annotations and labels as described below change the value of secretName For Certificate resources that are part of your deployments (e.g. to create a certificate that is mounted by an nginx sidecar for your main service): amend the Certificate 's annotations and labels change the value of secretName amend your deployment to mount the new secret (e.g. get the nginx sidecar to mount the new tls secret) Deploy the changes When you've checked that the service is functioning as intended, you can tidy up the old v0.8 cert-manager resources: delete any certificate resources still returned by kubectl -n project get certificate (back them up if they are not stored in git) delete any secrets associated with those old resources (again, back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificate.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. The main draw-back of this approach is that the value for the new secret being created will need to be created from LetsEncrypt (unless using the platform cluster issuer). This is usually quite quick, but could take up to around 2 minutes. During the time the new certificate is being requested and LetsEncrypt performs its http or dns challenge, your ingress will not have a valid certificate. So access to the endpoint is disrupted for that period whilst the challenge is being completed and new cert/secret generated. If you are keen on minimising service disruption further and only have current connections reset, please evaluate Option 2 below.","title":"Option 1 (renaming secrets)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#option-2-explicit-ingress-certificate","text":"This option is more complex than Option 1 and should only be considered if there are concerns with service availability while ingresses do not have a valid certifcate during the initial new certificate request. If not performed properly, you will gain nothing from it and it will have the same impact as Option 1. The high levels steps are: Leave the current Ingress resource as it is (whith old v0.8 annotations) Create a Certificate resource with letsencrypt-prod as the clusterIssuer, a secret name different from the one used by the Ingress and the appropriate stanzas as shown later on this guide. You might want to use the letsencrypt-staging clusterIssuer instead of letsencrypt-prod while changing your Certificate manifest file and testing it in order to not reach the weekly limits imposed by LetsEncrypt on its prod server and switch to letsencrypt-prod once you know your Certificate resource works as you expect. Deploy the changes to create the new certificate resource. Please note that the certificate and associated secret will at that point be unused, but make sure the Certificate is ready before deploying the next set of changes. You can check that by running kubectl get certificates.cert-manager.io in your namespace. Update your Ingress resources Remove all certmanager.k8s.io annotations DO NOT add any new cert-manager.io annotations or labels Update secretName in the Ingress resource to the name of the secret you created in step 2 (the secret associated with your Certficate resource) Deploy the Ingress changes When you've checked that the service is functioning as intended and that its certificate has been updated, you can tidy up the old v0.8 cert-manager resources and secrets: delete any certificate resources still returned by kubectl -n project get certificates.certmanager.k8s.io (back them up if they are not stored in git) delete any secrets associated with those old resources (again, back them up to be safe) You can check that the certificate resources are valid by running kubectl -n project get certificates.cert-manager.io . The READY field for the resources should be TRUE . Note that it might take a short while (typically no more than about a couple of minutes) for the certificates to reach that READY state. Please note that during the development lifecycle, you will quite naturally deploy the 2 changes above when they are made in 2 separate commit points. However, when it comes to deploying to other environments once the commits already exist, make sure to deploy the commit associated with the first step ( Certificate creation), wait for the Certificate resource to be ready and only deploy the change to the ingress once it is. If you do not wait and deploy those changes in quick succession, the outcome will be the same as for option 1: your service will be unavailable as it will not have a valid certificate until letsencrypt returns a new one.","title":"Option 2 (explicit ingress certificate)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#getting-cert-manager-resources","text":"As cert-manager is being upgraded from v0.8 to v0.13.1 and in order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io","title":"Getting cert-manager resources"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#updating-cert-manager-resources-for-v0131","text":"The following examples are based on the kube-example project.","title":"Updating cert-manager resources for v0.13.1"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#option-1-changes-recommended","text":"","title":"Option 1 changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#external-ingress-changes-recommended","text":"Changes required for websites or services exposed externally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: certmanager.k8s.io/acme-challenge-type: \"http01\" certmanager.k8s.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: change the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#internal-ingress-changes-recommended","text":"Changes required for websites or services exposed internally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: certmanager.k8s.io/acme-challenge-type: \"dns01\" certmanager.k8s.io/enabled: \"true\" certmanager.k8s.io/acme-dns01-provider: \"route53\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be changed to Ingress resource with the following v0.11+ annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: make sure you DON'T have the following annotation: kubernetes.io/tls-acme: \"true\" # @Note: change the enabled annotation to cert-manager.io/enabled cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" # @Note: add label cert-manager.io/solver to specify that the route53 dns01 solver should be used labels: cert-manager.io/solver: route53 spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio","title":"Internal Ingress changes (recommended)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#option-2-changes","text":"","title":"Option 2 changes"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#external-ingress-changes-2-stages","text":"Changes required for websites or services exposed externally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: certmanager.k8s.io/acme-challenge-type: \"http01\" certmanager.k8s.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-external-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-external-tls-cmio spec: secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_EXTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-external annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: no cert-manager.io annotations or labels are added ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-external\" spec: rules: - host: {{ .APP_HOST_EXTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_EXTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-external-tls-cmio","title":"External Ingress changes (2 stages)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#internal-ingress-changes-2-stages","text":"Changes required for websites or services exposed internally The following Ingress resource with v0.8 annotations: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: certmanager.k8s.io/acme-challenge-type: \"dns01\" certmanager.k8s.io/enabled: \"true\" certmanager.k8s.io/acme-dns01-provider: \"route53\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} secretName: {{ .DEPLOYMENT_NAME }}-internal-tls should be initially left unchanged. Deploy a new certificate apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio labels: cert-manager.io/solver: route53 spec: secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio issuerRef: # use letsencrypt-staging while developing and testing your certificates name: letsencrypt-prod kind: ClusterIssuer group: cert-manager.io dnsNames: - {{ .APP_HOST_INTERNAL }} Once the certificate is ready (run kubectl get certificates.cert-manager.io to find out its state), change and deploy the Ingress resource to specify the new secret name: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: {{ .DEPLOYMENT_NAME }}-internal annotations: # @Note: get rid of any certmanager.k8s.io annotations # @Note: no cert-manager.io annotations or labels are added cert-manager.io/enabled: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/force-ssl-redirect: \"true\" kubernetes.io/ingress.class: \"nginx-internal\" spec: rules: - host: {{ .APP_HOST_INTERNAL }} http: paths: - backend: serviceName: {{ .DEPLOYMENT_NAME }} servicePort: 10443 path: / tls: - hosts: - {{ .APP_HOST_INTERNAL }} # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-internal-tls-cmio","title":"Internal Ingress changes (2 stages)"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#certificate-resources-changes","text":"Please note that because the apiGroup for the new certificate resource ( cert-manager.io ) is different from the old one ( certmanager.k8s.io ), by making the changes below to your Certificate resource, you will actually create a new Certificate object as opposed to replacing the existing one. The following v0.8 Certificate resource providing a self-signed certificate: apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-service-tls spec: secretName: {{ .DEPLOYMENT_NAME }}-service-tls issuerRef: name: platform-tls kind: ClusterIssuer commonName: app.{{ .KUBE_NAMESPACE }}.svc.cluster.local dnsNames: - app - app.{{ .KUBE_NAMESPACE }}.svc should be changed to a v0.11 Certificate # @Note: change the apiVersion apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: {{ .DEPLOYMENT_NAME }}-service-tls spec: # @Note: change the secret name secretName: {{ .DEPLOYMENT_NAME }}-service-tls-cmio issuerRef: # @Note: change the name of the issuer name: platform-ca kind: ClusterIssuer commonName: app.{{ .KUBE_NAMESPACE }}.svc.cluster.local dnsNames: - app - app.{{ .KUBE_NAMESPACE }}.svc In order to convert a Certificate resource for a certificate issued by LetsEncrypt, the spec.issuerRef.name should be set as letsencrypt-prod . For an externally accessed service, no labelling is required. However for an internally accessed service whose certificate ACME challenge is resolved with dns01, the following label should be added: cert-manager.io/solver: route53 . For internal certificates issued by the platform-ca cluster issuer, please be aware of the following: if you use your certificate for client auth, you need to add a keyUsages section to your certificate (please see the main cert-manager guide for more detail) if you want to create a certificate for replicas in your statefulset, you should specify a DNS name of mysts-0.myservice.mynamespace to avoid having your resource being rejected by the admission policies (please see the main cert-manager guide for more detail)","title":"Certificate resources changes"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#network-policy-resources-changes","text":"The following NetworkPolicy resources used to be required to allow successful http01 ACME challenges for cert-manager v0.8 resources. This is no longer needed thanks to a new GlobalNetworkPolicy which has a cluster-wide scope. Network policies such as the following in your namespaces can therefore be deleted # No longer required; this NetworkPolicy can now be deleted kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: permit-certmanager-acme spec: policyTypes: - Ingress podSelector: matchExpressions: - {key: certmanager.k8s.io/acme-http-domain, operator: Exists} - {key: certmanager.k8s.io/acme-http-token, operator: Exists} ingress: - from: - namespaceSelector: matchLabels: name: ingress-external ports: - protocol: TCP port: 8089","title":"Network Policy resources changes"},{"location":"how-to-docs/cert-manager-upgrade-from-v0.8.html#deployment-verification","text":"Once you have applied the changes above, you should no longer have Certificate resources managed by v0.8. To verify that's the case, run kubectl get certificate -n my-namespace . The resource list returned should be empty. To verify that the new version of cert-manager is managing the certificates, run kubectl get certificate.cert-manager.io -n my-namespace . An non-empty list of resources should be returned, including any certificate resources you have created yourselves as well as ones created by cert-manager on behalf of an ingress. Once the development teams have migrated all their resources to the new version of cert-manager, the old instance will be decommissioned. When that's done it will no longer be needed to append .cert-manager.io to the resource kinds when using kubectl .","title":"Deployment verification"},{"location":"how-to-docs/cert-manager.html","text":"Cert Manager # VERY IMPORTANT upgrade information # cert-manager is being upgraded from v0.8 to v0.13.1. If you have cert-manager resources deployed in your namespaces, you MUST follow the instructions to upgrade from v0.8 to upgrade annotations and labels in order for them to be managed by the new version of cert-manager. To find out if you are using v0.8 cert-manager resources in your namespace, you can run: kubectl get certificates.certmanager.k8s.io Also LetsEncrypt will no longer be supporting PSG's kube-cert-manager from June 2020. So if you are using PSG kube-cert-manager to obtain certificates for your ingresses, you also need to migrate to JetStack's cert-manager v0.13.1 and follow the instructions to upgrade from PGS's kube-cert-manager To find out if you are using PSG kube-cert-manager to manage your ingresses certificates, you can run: kubectl get ingresses -o yaml | grep stable.k8s.psg.io Please also be aware that admission policies have been updated and will reject Ingress resources with annotations or labels supported by more than one certificate manager. There are currently ingresses with both cert-manager v0.8 and PSG annotations or labels and those will now fail applying. Background # The ACP platform presently has two certificate management services. The first service was PSG's kube-cert-manager . However with the forever changing landscape the project gradually became deprecated and now recommends replacement with JetStack's cert-manager . Therefore, projects still using kube-cert-manager should modify their services to start using cert-manager instead. Note that ACP will continue to support kube-cert-manager and the internal cfssl service while they are still in use, but we do recommend shifting over to cert-manager as soon as possible as aside from security fixes there will not be any more updates to these services. Without wishing to duplicate documentation which can be found in the readme and or official documentation , cert-manager can effectively replace two services: kube-cert-manager: used to acquire certificates from LetsEncrypt. cfssl: an internal Cloudflare service used to generate internal certificate (usually to encrypt between ingress and pod) . IMPORTANT NOTE: cert-manager is being upgraded from v0.8 to v0.13.1. In order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io How-tos # As a developer I already have a certificate from the legacy kube-cert-manager, how do I migrate? # Migrating from the former kube-cert-manager over to cert-manager means creating the certificate request as below and removing the annotations from the ingress. However, the safe way would be to; Create a new Certificate resource and point to a new secret name (thus keeping the old one incase) . Push out the change and wait for the certificate to be fulfilled. Once you have the certificate you can update your ingress to use the new secret, remove the annotations and use the Certificate resource thereafter. As a developer I want to retrieve an internal certificate # As stated above the cert-manager can also handle internal certificates i.e. those signed by the internal ACP Certificate Authority (this is self signed btw) . At the moment you might be using cfssl-sidekick to perform this, but this can be completely replaced. If you want to create a certificate for a service, assuming the service is called myservice in namespace mynamespace , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Ingress resources are checked by admission policies to ensure the platform-ca cluster issuer only issues certificates for DNS names that are hosted inside the namespace. So if you want to create a certificate for a replica in a statefulset, assuming your statufelset is called mysts , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - mysts-0.myservice.mynamespace - mysts-0.myservice.mynamespace.svc - mysts-0.myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Note that mysts-0.myservice is intentionally missing from the list in dnsNames because those names need to be either a hostname (for the service) or a name ending with mynamespace , mynamespace.svc or mynamespace.svc.cluster.local . This would create a kubernetes secret named tls in your namespace with the signed certificate. An interesting thing to note here is that although this is using the ClusterIssuer platform-ca created by the ACP team, there is nothing stopping a project from creating a local Issuer for their own project. So for example. --- apiVersion: cert-manager.io/v1alpha2 kind: Issuer metadata: name: project-ca spec: ca: secretName: project-ca --- apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: project-ca # @Note: we have change from ClusterIssuer to a local Issuer kind: Issuer commonName: site.svc.project.cluster.local dnsNames: - localhost ipAddresses: - 127.0.0.1 Finally, if you want to use your certificate for client auth (as well as server auth in the following example), you need to add a keyUsages section to your Certificate resource: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer keyUsages: - server auth - client auth dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 As a developer I want to retrieve a certificate for my external service # Let's assume we have an externally facing site which we wish to expose via ingress and we want a valid LetsEncrypt certificate. Getting a certificate associated with the external ingress only requires to annotate the ingress with cert-manager.io/enabled , which is a toggle to ask cert-manager to handle this ingress resource. Optionally, the acme solver to be used by the cluster issuer can be specified with label cert-manager.io/solver: http01 . However, this is not required as the http01 acme solver is the default one. Please note that cert-manager.io/enabled is an annotation but cert-manager.io/solver is a label. When the site is externally facing i.e. the ingress class on the ingress is kubernetes.io/ingress.class: nginx-external you should always default to using a http01 challenge. However, if you know that the domain whitelisted in your namespace is hosted in AWS by Route53, you can instead specify a label of cert-manager.io/solver: route53 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: # @NOTE: this will enable cert-manager to handle this resource cert-manager.io/enabled: \"true\" ingress.kubernetes.io/affinity: cookie ingress.kubernetes.io/force-ssl-redirect: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/session-cookie-name: ingress ingress.kubernetes.io/ssl-redirect: \"true\" kubernetes.io/ingress.class: nginx-external name: example # @NOTE: the following label can be specified to ask letsencrypt to use the http01 acme challenge # @NOTE: but it is not required as http01 is the default solver labels: cert-manager.io/solver: http01 spec: rules: - host: www.example.com http: paths: - backend: serviceName: service_name servicePort: 10443 path: / tls: - hosts: - www.example.com # @NOTE: this is the name of the kubernetes secret that cert-manager will manage in your namespace secretName: example-tls A few things to note here: behind the scenes, cert-manager works with the Certificate custom resource. when using ingress annotations and labels, cert-manager uses another internal controller to pick up the ingress resources and create a Certificate resource on your behalf. Of course, you can instead define this directly yourself but you will also have to define annotations on the ingress resource to specify which secret should be used for TLS termination. This is the recommended and safest approach when migrating from kube-cert-manager to cert-manager . apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: commonName: www.example.com dnsNames: - www.example.com issuerRef: kind: ClusterIssuer # @Note: we support letsencrypt-prod and letsencrypt-staging (use the latter to test your cert-manager related manifests) name: letsencrypt-prod secretName: example-tls You can review, get, list and describe the Certificate like any other kubernetes resource within your namespace. $ kubectl -n project get certificate NAME AGE example-tls 1d # you can also review the certificaterequests, orders and challenges via $ kubectl -n project get orders $ kubectl -n project get challenge Network Policies Please note that as part of the implementation of cert-manager v0.13.1, a GlobalNetworkPolicy object managing ingress traffic for http01 challenges has been deployed. This means that you no longer need to have a NetworkPolicy in your namespaces allowing ingress traffic from port 8089 to the ephemeral pods that cert-manager creates to handle the http01 challenge. As a developer I want to retrieve a certificate for a service behind the vpn, or simply wish to use the DNS validation # When a site is internal / behind the vpn, in order to handle the challenge you need to switch to using a DNS challenge. This is done by adding the following to your ingress resource: annotation cert-manager.io/enabled: \"true\" label cert-manager.io/solver: route53 Very Important : in order to successfully switch to a DNS challenge, please ensure you have contacted the ACP team before attempting this for the first time on your sub-domain as the correct permissions need to exist to permit cert-manager to add records to the domain. apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example-tls labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: mysite.example.com dnsNames: - example.com acme: config: - dns01: provider: route53 domains: - mysite.example.com - example.com Or via ingress you would use apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: # @Note: get cert-manager to manage this ingress cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls As a developer I want to use LetsEncrypt staging while configuring my cert-manager resources # You should use the staging version of LetsEncrypt in order to not be impacted by rate limits of the production version while setting up and testing the cert-manager annotations and labels you specify on your resources. By default, the production version of the LetsEncrypt ACME servers is used. To use the staging version, use the cert-manager.io/cluster-issuer annotation: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal # @Note: we are specifying which cluster issuer to use cert-manager.io/cluster-issuer: letsencrypt-staging labels: cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls Not specifying this annotation is equivalent to specifying cert-manager.io/cluster-issuer: letsencrypt-prod . Please note that the certificates issued by the staging version of LetsEncrypt are not signed and should not be used in production. As a developer I want to get a certificate for a server with a DNS name longer than 63 characters # A certificate's commonName is used to create a Certificate Signing Request and populate a field that is limited to 63 characters. In order to get a certificate for a server with a DNS name longer than 63 characters, you need to specify a common name of less than 63 characters and add the desired DNS name as an additional entry to dnsNames . For example, with an Ingress : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: cert-manager.io/solver: route53 spec: rules: - host: my-rather-long-winded-service-name.my-namespace.subdomain.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com secretName: example-tls Or with a Certificate : apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: svc-1.my-namespace.subdomain.example.com dnsNames: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com","title":"**Cert Manager**"},{"location":"how-to-docs/cert-manager.html#cert-manager","text":"","title":"Cert Manager"},{"location":"how-to-docs/cert-manager.html#very-important-upgrade-information","text":"cert-manager is being upgraded from v0.8 to v0.13.1. If you have cert-manager resources deployed in your namespaces, you MUST follow the instructions to upgrade from v0.8 to upgrade annotations and labels in order for them to be managed by the new version of cert-manager. To find out if you are using v0.8 cert-manager resources in your namespace, you can run: kubectl get certificates.certmanager.k8s.io Also LetsEncrypt will no longer be supporting PSG's kube-cert-manager from June 2020. So if you are using PSG kube-cert-manager to obtain certificates for your ingresses, you also need to migrate to JetStack's cert-manager v0.13.1 and follow the instructions to upgrade from PGS's kube-cert-manager To find out if you are using PSG kube-cert-manager to manage your ingresses certificates, you can run: kubectl get ingresses -o yaml | grep stable.k8s.psg.io Please also be aware that admission policies have been updated and will reject Ingress resources with annotations or labels supported by more than one certificate manager. There are currently ingresses with both cert-manager v0.8 and PSG annotations or labels and those will now fail applying.","title":"VERY IMPORTANT upgrade information"},{"location":"how-to-docs/cert-manager.html#background","text":"The ACP platform presently has two certificate management services. The first service was PSG's kube-cert-manager . However with the forever changing landscape the project gradually became deprecated and now recommends replacement with JetStack's cert-manager . Therefore, projects still using kube-cert-manager should modify their services to start using cert-manager instead. Note that ACP will continue to support kube-cert-manager and the internal cfssl service while they are still in use, but we do recommend shifting over to cert-manager as soon as possible as aside from security fixes there will not be any more updates to these services. Without wishing to duplicate documentation which can be found in the readme and or official documentation , cert-manager can effectively replace two services: kube-cert-manager: used to acquire certificates from LetsEncrypt. cfssl: an internal Cloudflare service used to generate internal certificate (usually to encrypt between ingress and pod) . IMPORTANT NOTE: cert-manager is being upgraded from v0.8 to v0.13.1. In order to allow development teams to upgrade their cert-manager resources according to their own schedule, both v0.8 and v.13.1 resources will be available concurrently for a period of time. While the older version of cert-manager (v0.8) is still available on the ACP platform, resources managed by the newer version of cert-manager (v0.13.1+) can only be accessed from the API server by suffixing the resource kind with .cert-manager.io . For example: # to access v0.13.1 cert-manager resources kubectl -n project get certificate.cert-manager.io kubectl -n project get orders.acme.cert-manager.io kubectl -n project get challenge.acme.cert-manager.io # to access v0.8 cert-manager resources kubectl -n project get certificate kubectl -n project get orders kubectl -n project get challenge # or kubectl -n project get certificate.certmanager.k8s.io kubectl -n project get orders.certmanager.k8s.io kubectl -n project get challenge.certmanager.k8s.io","title":"Background"},{"location":"how-to-docs/cert-manager.html#how-tos","text":"","title":"How-tos"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-already-have-a-certificate-from-the-legacy-kube-cert-manager-how-do-i-migrate","text":"Migrating from the former kube-cert-manager over to cert-manager means creating the certificate request as below and removing the annotations from the ingress. However, the safe way would be to; Create a new Certificate resource and point to a new secret name (thus keeping the old one incase) . Push out the change and wait for the certificate to be fulfilled. Once you have the certificate you can update your ingress to use the new secret, remove the annotations and use the Certificate resource thereafter.","title":"As a developer I already have a certificate from the legacy kube-cert-manager, how do I migrate?"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-want-to-retrieve-an-internal-certificate","text":"As stated above the cert-manager can also handle internal certificates i.e. those signed by the internal ACP Certificate Authority (this is self signed btw) . At the moment you might be using cfssl-sidekick to perform this, but this can be completely replaced. If you want to create a certificate for a service, assuming the service is called myservice in namespace mynamespace , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Ingress resources are checked by admission policies to ensure the platform-ca cluster issuer only issues certificates for DNS names that are hosted inside the namespace. So if you want to create a certificate for a replica in a statefulset, assuming your statufelset is called mysts , the Certificate definition would look like: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer dnsNames: - mysts-0.myservice.mynamespace - mysts-0.myservice.mynamespace.svc - mysts-0.myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1 Note that mysts-0.myservice is intentionally missing from the list in dnsNames because those names need to be either a hostname (for the service) or a name ending with mynamespace , mynamespace.svc or mynamespace.svc.cluster.local . This would create a kubernetes secret named tls in your namespace with the signed certificate. An interesting thing to note here is that although this is using the ClusterIssuer platform-ca created by the ACP team, there is nothing stopping a project from creating a local Issuer for their own project. So for example. --- apiVersion: cert-manager.io/v1alpha2 kind: Issuer metadata: name: project-ca spec: ca: secretName: project-ca --- apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: project-ca # @Note: we have change from ClusterIssuer to a local Issuer kind: Issuer commonName: site.svc.project.cluster.local dnsNames: - localhost ipAddresses: - 127.0.0.1 Finally, if you want to use your certificate for client auth (as well as server auth in the following example), you need to add a keyUsages section to your Certificate resource: apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: tls spec: secretName: tls issuerRef: name: platform-ca kind: ClusterIssuer keyUsages: - server auth - client auth dnsNames: - myservice - myservice.mynamespace - myservice.mynamespace.svc - myservice.mynamespace.svc.cluster.local - localhost ipAddresses: - 127.0.0.1","title":"As a developer I want to retrieve an internal certificate"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-want-to-retrieve-a-certificate-for-my-external-service","text":"Let's assume we have an externally facing site which we wish to expose via ingress and we want a valid LetsEncrypt certificate. Getting a certificate associated with the external ingress only requires to annotate the ingress with cert-manager.io/enabled , which is a toggle to ask cert-manager to handle this ingress resource. Optionally, the acme solver to be used by the cluster issuer can be specified with label cert-manager.io/solver: http01 . However, this is not required as the http01 acme solver is the default one. Please note that cert-manager.io/enabled is an annotation but cert-manager.io/solver is a label. When the site is externally facing i.e. the ingress class on the ingress is kubernetes.io/ingress.class: nginx-external you should always default to using a http01 challenge. However, if you know that the domain whitelisted in your namespace is hosted in AWS by Route53, you can instead specify a label of cert-manager.io/solver: route53 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: # @NOTE: this will enable cert-manager to handle this resource cert-manager.io/enabled: \"true\" ingress.kubernetes.io/affinity: cookie ingress.kubernetes.io/force-ssl-redirect: \"true\" ingress.kubernetes.io/backend-protocol: \"HTTPS\" ingress.kubernetes.io/session-cookie-name: ingress ingress.kubernetes.io/ssl-redirect: \"true\" kubernetes.io/ingress.class: nginx-external name: example # @NOTE: the following label can be specified to ask letsencrypt to use the http01 acme challenge # @NOTE: but it is not required as http01 is the default solver labels: cert-manager.io/solver: http01 spec: rules: - host: www.example.com http: paths: - backend: serviceName: service_name servicePort: 10443 path: / tls: - hosts: - www.example.com # @NOTE: this is the name of the kubernetes secret that cert-manager will manage in your namespace secretName: example-tls A few things to note here: behind the scenes, cert-manager works with the Certificate custom resource. when using ingress annotations and labels, cert-manager uses another internal controller to pick up the ingress resources and create a Certificate resource on your behalf. Of course, you can instead define this directly yourself but you will also have to define annotations on the ingress resource to specify which secret should be used for TLS termination. This is the recommended and safest approach when migrating from kube-cert-manager to cert-manager . apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: commonName: www.example.com dnsNames: - www.example.com issuerRef: kind: ClusterIssuer # @Note: we support letsencrypt-prod and letsencrypt-staging (use the latter to test your cert-manager related manifests) name: letsencrypt-prod secretName: example-tls You can review, get, list and describe the Certificate like any other kubernetes resource within your namespace. $ kubectl -n project get certificate NAME AGE example-tls 1d # you can also review the certificaterequests, orders and challenges via $ kubectl -n project get orders $ kubectl -n project get challenge Network Policies Please note that as part of the implementation of cert-manager v0.13.1, a GlobalNetworkPolicy object managing ingress traffic for http01 challenges has been deployed. This means that you no longer need to have a NetworkPolicy in your namespaces allowing ingress traffic from port 8089 to the ephemeral pods that cert-manager creates to handle the http01 challenge.","title":"As a developer I want to retrieve a certificate for my external service"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-want-to-retrieve-a-certificate-for-a-service-behind-the-vpn-or-simply-wish-to-use-the-dns-validation","text":"When a site is internal / behind the vpn, in order to handle the challenge you need to switch to using a DNS challenge. This is done by adding the following to your ingress resource: annotation cert-manager.io/enabled: \"true\" label cert-manager.io/solver: route53 Very Important : in order to successfully switch to a DNS challenge, please ensure you have contacted the ACP team before attempting this for the first time on your sub-domain as the correct permissions need to exist to permit cert-manager to add records to the domain. apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example-tls labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: mysite.example.com dnsNames: - example.com acme: config: - dns01: provider: route53 domains: - mysite.example.com - example.com Or via ingress you would use apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: # @Note: get cert-manager to manage this ingress cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: # @Note: this label tells the cluster issuer to use the DNS01 Route53 solver instead of the default HTTP01 solver cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls","title":"As a developer I want to retrieve a certificate for a service behind the vpn, or simply wish to use the DNS validation"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-want-to-use-letsencrypt-staging-while-configuring-my-cert-manager-resources","text":"You should use the staging version of LetsEncrypt in order to not be impacted by rate limits of the production version while setting up and testing the cert-manager annotations and labels you specify on your resources. By default, the production version of the LetsEncrypt ACME servers is used. To use the staging version, use the cert-manager.io/cluster-issuer annotation: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal # @Note: we are specifying which cluster issuer to use cert-manager.io/cluster-issuer: letsencrypt-staging labels: cert-manager.io/solver: route53 spec: rules: - host: mysite.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - mysite.example.com - example.com secretName: example-tls Not specifying this annotation is equivalent to specifying cert-manager.io/cluster-issuer: letsencrypt-prod . Please note that the certificates issued by the staging version of LetsEncrypt are not signed and should not be used in production.","title":"As a developer I want to use LetsEncrypt staging while configuring my cert-manager resources"},{"location":"how-to-docs/cert-manager.html#as-a-developer-i-want-to-get-a-certificate-for-a-server-with-a-dns-name-longer-than-63-characters","text":"A certificate's commonName is used to create a Certificate Signing Request and populate a field that is limited to 63 characters. In order to get a certificate for a server with a DNS name longer than 63 characters, you need to specify a common name of less than 63 characters and add the desired DNS name as an additional entry to dnsNames . For example, with an Ingress : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: example annotations: cert-manager.io/enabled: \"true\" kubernetes.io/ingress.class: nginx-internal labels: cert-manager.io/solver: route53 spec: rules: - host: my-rather-long-winded-service-name.my-namespace.subdomain.example.com http: paths: - backend: serviceName: service_name servicePort: 443 path: / tls: - hosts: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com secretName: example-tls Or with a Certificate : apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: example spec: secretName: example-tls issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: svc-1.my-namespace.subdomain.example.com dnsNames: - svc-1.my-namespace.subdomain.example.com - my-rather-long-winded-service-name.my-namespace.subdomain.example.com","title":"As a developer I want to get a certificate for a server with a DNS name longer than 63 characters"},{"location":"how-to-docs/chisel.html","text":"Chisel # The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination. A Working Example # We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:v1.3.1 # Both Chisel Client & Server versions must match securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point.","title":"Chisel"},{"location":"how-to-docs/chisel.html#chisel","text":"The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination.","title":"Chisel"},{"location":"how-to-docs/chisel.html#a-working-example","text":"We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:v1.3.1 # Both Chisel Client & Server versions must match securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point.","title":"A Working Example"},{"location":"how-to-docs/debug-issues.html","text":"Debug Issues with your deployments # Debug with secrets # Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set. Debugging issues with your deployments to the platform # If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application. Debugging deployments # We suggest the following steps: 1. Check your deployment, replicaset and pods created properly # $ kubectl get deployments $ kubectl get rs $ kubectl get pods 2. Investigate potential issues with your pods (this is most likely) # If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.* 3. Investigate potential issues with your service # A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name 4. Investigate potential issues with ingress # Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti <ingress-3879072234-5f4uq> -c <proxy> bash where <proxy> is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs <ingress-3879072234-5f4uq>","title":"Debug issues"},{"location":"how-to-docs/debug-issues.html#debug-issues-with-your-deployments","text":"","title":"Debug Issues with your deployments"},{"location":"how-to-docs/debug-issues.html#debug-with-secrets","text":"Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set.","title":"Debug with secrets"},{"location":"how-to-docs/debug-issues.html#debugging-issues-with-your-deployments-to-the-platform","text":"If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application.","title":"Debugging issues with your deployments to the platform"},{"location":"how-to-docs/debug-issues.html#debugging-deployments","text":"We suggest the following steps:","title":"Debugging deployments"},{"location":"how-to-docs/debug-issues.html#1-check-your-deployment-replicaset-and-pods-created-properly","text":"$ kubectl get deployments $ kubectl get rs $ kubectl get pods","title":"1. Check your deployment, replicaset and pods created properly"},{"location":"how-to-docs/debug-issues.html#2-investigate-potential-issues-with-your-pods-this-is-most-likely","text":"If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.*","title":"2. Investigate potential issues with your pods (this is most likely)"},{"location":"how-to-docs/debug-issues.html#3-investigate-potential-issues-with-your-service","text":"A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name","title":"3. Investigate potential issues with your service"},{"location":"how-to-docs/debug-issues.html#4-investigate-potential-issues-with-ingress","text":"Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti <ingress-3879072234-5f4uq> -c <proxy> bash where <proxy> is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs <ingress-3879072234-5f4uq>","title":"4. Investigate potential issues with ingress"},{"location":"how-to-docs/dms-migration.html","text":"DMS Migration # Prerequisite # The following need to be true before you follow this guide: * AWS console logon * Access to the DMS service from console * A region where STS has been activated DMS Setup # Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button.","title":"Dms migration"},{"location":"how-to-docs/dms-migration.html#dms-migration","text":"","title":"DMS Migration"},{"location":"how-to-docs/dms-migration.html#prerequisite","text":"The following need to be true before you follow this guide: * AWS console logon * Access to the DMS service from console * A region where STS has been activated","title":"Prerequisite"},{"location":"how-to-docs/dms-migration.html#dms-setup","text":"Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button.","title":"DMS Setup"},{"location":"how-to-docs/downscaling.html","text":"Downscaling Services Out Of Hours # In an effort to reduce costs on running the platform, we've enabled the capability to scale down specific resources Out Of Hours (OOH) for Non-Production and Production environments. AWS RDS (Relational Database Service) # RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Support Portal support request template . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage. Kubernetes Pods # Automatically scale down Kubernetes Deployments & Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments & Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters. Usage Set ONE of the following annotations on your Deployment / Statefulset: - downscaler/uptime : A time schedule in which the Deployment should be scaled up - downscaler/downtime : A time schedule in which the Deployment should be scaled down to 0 replicas The annotation values for the timeframe must have the following format to be processed correctly: <WEEKDAY-FROM>-<WEEKDAY-TO-INCLUSIVE> <HH>:<MM>-<HH>:<MM> <TIMEZONE> For example, to schedule a Deployment to only run on weekdays during working hours, the following annotation would be set: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London Note: When the deployment is downscaled, an additional annotation downscaler/original-replicas is automatically set to retain a history of the desired replicas prior to the downscale action. If this annotation has been deleted before the service is automatically scaled back up, the downscaler service will not know what to set the replicas back to, and so it won't attempt to scale up the resource. Example Spec: apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London labels: name: example-app name: example-app namespace: acp-example spec: replicas: 2 template: spec: containers: image: docker.digital.homeoffice.gov.uk/acp-example-app:v0.0.1@sha256:07397c41ac25c4b19e0485006849201f04168703f0016fad75b8ba5d9885d6d4 ...","title":"Downscaling"},{"location":"how-to-docs/downscaling.html#downscaling-services-out-of-hours","text":"In an effort to reduce costs on running the platform, we've enabled the capability to scale down specific resources Out Of Hours (OOH) for Non-Production and Production environments.","title":"Downscaling Services Out Of Hours"},{"location":"how-to-docs/downscaling.html#aws-rds-relational-database-service","text":"RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Support Portal support request template . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage.","title":"AWS RDS (Relational Database Service)"},{"location":"how-to-docs/downscaling.html#kubernetes-pods","text":"Automatically scale down Kubernetes Deployments & Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments & Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters.","title":"Kubernetes Pods"},{"location":"how-to-docs/drone-how-to.html","text":"Drone How To # Install Drone CLI # Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN=<your_drone_token> If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly. Activate your pipeline # Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Sync your repository access rights with Drone by clicking the icon in the top right corner again with the Synchronize button - this needs to be applied everytime when a new repository is created. Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo. Configure your pipeline # In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name <...> with the name of your app. You should be able to watch your build succeed in the Drone UI. Publishing Docker images # Publishing to Quay If your repository is hosted on Gitlab, you don't want to publish your images to Quay. Images published to Quay are public and can be inspected and downloaded by anyone. You should publish your private images to Artifactory . Register for a free Quay account using your Github account linked to the Home Office organisation. Once you've logged into Quay check that you have ukhomeofficedigital under Users and Organisations. If you do not, submit a support request on the support portal for access to the ukhomeoffice organisation . Once you have access to view the ukhomeofficedigital repositories, click repositories and click the + Create New Repositories that is: public empty - no need to create a repo from a Dockerfile or link it to an existing repository Add your project to the UKHomeOffice Quay account and submit a support request on the support portal for a new Quay robot . Add the step to publish the docker image to Quay in your Drone pipeline: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where <your_quay_repo> in: quay.io/ukhomeofficedigital/<your_quay_repo> is the name of the Quay repo you (should) have already created. Note: ${DRONE_COMMIT_SHA} is a Drone environment variable that is passed to the container at runtime. The build should fail with the following error: Error response from daemon: Get https://quay.io/v2/: unauthorized: Could not find robot with username: ukhomeofficedigital+<your_robot_username> and supplied password. The error points to the missing password for the Quay robot. You will need to add this as a drone secret. You can do this through the Drone UI by going to your repo, clicking the menu icon in the top right and then clicking Secrets . You should be presented with a list of the secrets for that repo (if there are any) and you should be able to add secrets giving them a name and value. Add a secret with the name DOCKER_PASSWORD and with the value being the robot token that was supplied to you. Alternatively, you can use the Drone CLI to add the secret: $ drone secret add --repository ukhomeoffice/<your_github_repo> --name DOCKER_PASSWORD --value <your_robot_token> Restarting the build should be enough to make it pass. The Drone CLI allows for more control over the secret as opposed to the UI. For example, the CLI allows you to specify the image and the events that the secret will be allowed to be used with. Also note that the secret was specified in the secrets section of the pipeline to give it access to the secret. Without this, the pipeline would not be able to use the secret and it would fail. Secrets in this section are automatically uppercased at runtime so it is important that the secret is uppercased in your commands. You can also push specifically tagged images by using the DRONE_TAG Drone environment variable and by using the tag event: tagged_image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - ${DRONE_TAG} when: event: tag Tag using git tag v1.0 and push your tag with git push origin v1.0 (replace v1.0 with the tag you actually want to use). Note: These pipeline configurations are using the Docker plugin for Drone. For more information, see http://plugins.drone.io/drone-plugins/drone-docker/ Publishing to Artifactory Images hosted on Artifactory are private. If your repository is hosted publicly on GitHub, you shouldn't publish your images to Artifactory. Artifactory is only used to publish private images. You should use Quay to publish your public images . Submit a support request for a new Artifactory access token . You should be supplied an access token in response. You can inject the token that has been supplied to you with: $ drone secret add --repository <gitlab_repo_group>/<your_gitlab_repo> --name DOCKER_PASSWORD --value <your_robot_token> You can add the following step in your .drone.yml : image_to_artifactory: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=<your_robots_username> registry: docker.digital.homeoffice.gov.uk repo: docker.digital.homeoffice.gov.uk/<your_artifactory_repo> tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where the <image_name> in: docker tag <image_name> docker.digital.homeoffice.gov.uk/ukhomeofficedigital/<your_artifactory_repo>:$${DRONE_COMMIT_SHA} is the name of the image you tagged previously in the build step. The image should now be published on Artifactory. Please note that we regularly remove container images that have not been downloaded in a year. Deployments # Deployments and promotions Create a step that runs only on deployments: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment Push the changes to your remote repository. You can deploy the build you just pushed with the following command: $ drone deploy ukhomeoffice/<your_repo> 16 preprod Where 16 is the successful build number on drone that you wish to deploy to the preprod environment. You can pass additional parameters to your deployment as environment variables: $ drone deploy ukhomeoffice/<your_repo> 16 preprod -p DEBUG=1 -p NAME=Dan and use them in the step like this: deploy-to-preprod: image: busybox commands: - /bin/echo hello $${NAME} when: environment: preprod event: deployment Environments are strings and can be set to any value. When you wish to deploy to several environments you can create a step for each one of them: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment deploy-to-prod: image: busybox commands: - /bin/echo hello prod when: environment: prod event: deployment And deploy them accordingly: $ drone deploy ukhomeoffice/<your_repo> 16 preprod $ drone deploy ukhomeoffice/<your_repo> 16 prod Read more on environments . Drone as a Pull Request builder Drone pipelines are triggered when events occurs. Event triggers can be as simple as a push , a tagged commit , a pull request or as granular as only for pull requests for a branch named test . You can limit the execution of build steps at runtime using the when block. As an example, this block executes only on pull requests: pr-builder: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: event: pull_request Drone will only execute that step when a new pull request is raised (and when pushes are made to the branch while a pull request is open). Read more about Drone conditions . Deploying to ACP Please note that this section assumes that you already have kube files to work with (specifically, deployment, service and ingress files). Examples of these files can be found in the kube-signed-commit-check project. Add a deployment script with the following: #!/bin/bash export KUBE_NAMESPACE=<dev-induction> export KUBE_SERVER=${KUBE_SERVER} export KUBE_TOKEN=${KUBE_TOKEN} kd -f deployment.yaml \\ -f service.yaml \\ -f ingress.yaml Please note that this is only an example script and it will need to be changed to fit your particular application's needs. If you deployed this now you would likely receive an error similar to this: error: You must be logged in to the server (the server has asked for the client to provide credentials) This error appears because kd needs 3 environment variables to be set before deploying: KUBE_NAMESPACE - The kubernetes namespace you wish to deploy to. You need to provide the kubernetes namespace as part of the deployment job . KUBE_TOKEN - This is the token used to authenticate against the kubernetes cluster. If you do not already have a kube token, here are docs explaining how to get one . KUBE_SERVER - This is the address of the kubernetes cluster that you want to deploy to. You will need to add KUBE_TOKEN and KUBE_SERVER as drone secrets. Information about how to add Drone secrets can be found in the publishing to Quay section . You can verify that the secrets for your repo are present with: $ drone secret ls --repository ukhomeoffice/<your-repo> Once the secrets have been added, add a new step to your drone pipeline that will execute the deployment script: deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - ./deploy.sh when: environment: uat event: deployment Using Another Repo # It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/<deployment_repo> branch: <master> deploy_to: <uat> params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/<your_repo> 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs. Versioned deployments # When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update && apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment Migrating your pipeline # Secrets and Signing It is no longer necessary to sign your .drone.yml so the .drone.yml.sig can be deleted. Secrets can be defined in the Drone UI or using the CLI. Secrets created using the UI will be available to push, tag and deployment events. To restrict to selected events, or to allow pull request builds to access secrets you must use the CLI. Pipelines by default do not have access to any Drone secrets that you have added. You must now define which secrets a pipeline is allowed access to in a secrets section in your pipeline. Here is an example of a pipeline that has access to the DOCKER_PASSWORD secret which will be used to push an image to Quay: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+<your_robot_username> registry: quay.io repo: quay.io/ukhomeofficedigital/<your_quay_repo> tags: - latest when: branch: master event: push Note: Secrets names in the secrets section will have their names uppercased at runtime. Organisation secrets are no longer available. This means that if you are using any organisation secrets such as KUBE_TOKEN_DEV , you will need to add a secret in Drone to replace it. Docker-in-Docker # The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push Services # If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306 Variable Escaping # Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} --> $${DOCKER_PASSWORD} ${DRONE_TAG} --> $${DRONE_TAG} ${DRONE_COMMIT_SHA} --> $${DRONE_COMMIT_SHA} Scanning Images in Drone # ACP provides Anchore as a scanning solution for images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well as any public images. Example pipeline: pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables) image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # Indicates the image is locally available local_image: true # This indicates we are willing tolerate any vulnerabilities which are below medium (valid values: negligible, low, medium, high, critical) tolerate: medium # An optional whitelist (comma separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: <PATH> # Indicates we should show all vulnerabilities regardless show_all_vulnerabilities: false # By default the plugin will exit will fail if any vulnerabilities are discovered which are not tolerated, # you change this behaviour by setting the below fail_on_detection: false Q&As # Q: The build fails with \"ERROR: Insufficient privileges to use privileged mode\" A: Remove privileged: true from your .drone.yml . As explained in the migrating your pipeline section , the primary use of this was for Docker-in-Docker which is not required. Q: The build fails with \"Cannot connect to the Docker daemon. Is the docker daemon running on this host?\" A: Make sure that your steps contain the environment variable DOCKER_HOST=tcp://172.17.0.1:2375 like in this case: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Q: The build fails when uploading to Quay with the error \"Error response from daemon: Get https://quay.io/v2/: unauthorized:...\" A: This is likely because the secret wasn't added correctly or the password is incorrect. Check that the secret has been added to Drone and that you have added the secrets section in your .drone.yaml it to the pipeline that requires it. Q: As part of my build process I have two Dockerfiles to produce a Docker image. How can I share files between builds in the same step? A: When the pipeline starts, Drone creates a Docker data volume that is passed along all active steps in the pipeline. If the first step creates a test.txt file, the second step can use that file. As an example, this pipeline uses a two step build process: pipeline: first-step: image: busybox commands: - echo hello > test.txt when: branch: master event: push second-step: image: busybox commands: - cat test.txt when: branch: master event: push Q: Should I use Gitlab with Quay? A: Please don't. If your repository is hosted in Gitlab then use Artifactory to publish your images. Images published to Artifactory are kept private. If you still want to use Quay, you should consider hosting your repository on the open (Github). Q: Can I create a token that has permission to create ephemeral/temporary namespaces? A: No. This is because there is currently no way to give access to namespaces via regex. I.e. There is no way to give access to any namespace with the format: my-temp-namespace-* (where * would be build number or something similar). Alternatively, you can be given a named namespace in the CI cluster. Please create an issue on our Support Portal if you require this.","title":"Drone how to"},{"location":"how-to-docs/drone-how-to.html#drone-how-to","text":"","title":"Drone How To"},{"location":"how-to-docs/drone-how-to.html#install-drone-cli","text":"Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN=<your_drone_token> If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly.","title":"Install Drone CLI"},{"location":"how-to-docs/drone-how-to.html#activate-your-pipeline","text":"Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Sync your repository access rights with Drone by clicking the icon in the top right corner again with the Synchronize button - this needs to be applied everytime when a new repository is created. Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo.","title":"Activate your pipeline"},{"location":"how-to-docs/drone-how-to.html#configure-your-pipeline","text":"In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t <image_name> . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name <...> with the name of your app. You should be able to watch your build succeed in the Drone UI.","title":"Configure your pipeline"},{"location":"how-to-docs/drone-how-to.html#publishing-docker-images","text":"","title":"Publishing Docker images"},{"location":"how-to-docs/drone-how-to.html#deployments","text":"","title":"Deployments"},{"location":"how-to-docs/drone-how-to.html#using-another-repo","text":"It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/<deployment_repo> branch: <master> deploy_to: <uat> params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/<your_repo> 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs.","title":"Using Another Repo"},{"location":"how-to-docs/drone-how-to.html#versioned-deployments","text":"When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/<your_repo>.git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update && apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment","title":"Versioned deployments"},{"location":"how-to-docs/drone-how-to.html#migrating-your-pipeline","text":"","title":"Migrating your pipeline"},{"location":"how-to-docs/drone-how-to.html#docker-in-docker","text":"The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push","title":"Docker-in-Docker"},{"location":"how-to-docs/drone-how-to.html#services","text":"If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306","title":"Services"},{"location":"how-to-docs/drone-how-to.html#variable-escaping","text":"Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} --> $${DOCKER_PASSWORD} ${DRONE_TAG} --> $${DRONE_TAG} ${DRONE_COMMIT_SHA} --> $${DRONE_COMMIT_SHA}","title":"Variable Escaping"},{"location":"how-to-docs/drone-how-to.html#scanning-images-in-drone","text":"ACP provides Anchore as a scanning solution for images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well as any public images. Example pipeline: pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables) image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # Indicates the image is locally available local_image: true # This indicates we are willing tolerate any vulnerabilities which are below medium (valid values: negligible, low, medium, high, critical) tolerate: medium # An optional whitelist (comma separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: <PATH> # Indicates we should show all vulnerabilities regardless show_all_vulnerabilities: false # By default the plugin will exit will fail if any vulnerabilities are discovered which are not tolerated, # you change this behaviour by setting the below fail_on_detection: false","title":"Scanning Images in Drone"},{"location":"how-to-docs/drone-how-to.html#qas","text":"","title":"Q&amp;As"},{"location":"how-to-docs/elastic-container-registry.html","text":"AWS ECR for Private Docker Images # AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials. Creating a Docker Repository # Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates. Generating Access Credentials # Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it. Accessing a Docker Repository # Accessing the AWS Container Registry to Pull & Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated. Pre-Requisites # To follow the below steps you must have: * AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows * Docker (version 17.06 or above, check with docker --version ) Step 1: Retrieve an authorisation token # Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com Step 2: Login with Authorisation Token # Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Steps 1 and 2 will also not work if you are using AWS CLI (version 2.*). Instead use $ aws_account_id=\"340268328991\" $ aws_region=\"eu-west-2\" $ ecr_url=\"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" $ aws --region \"${aws_region}\" ecr get-login-password \\ | docker login \\ --password-stdin \\ --username AWS \\ \"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" Login Succeeded Pulling & Pushing Images # Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Listing Images & Housekeeping # Using the AWS CLI you can list all images that have been pushed to a given repository which you have access to. For example: $ aws ecr list-images --repository-name acp/hello-world-app { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" }, { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ] } To delete old images, you must have write access enabled or perform the action via a Robot Account. Images can be deleted based on a provided tag or digest. When providing a digest, all image tags with the same digest are deleted together. $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageTag=v0.0.1 { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" } ], \"failures\": [] } $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageDigest=sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ], \"failures\": [] } Managing Image Deployments via Drone CI # The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI.","title":"Elastic container registry"},{"location":"how-to-docs/elastic-container-registry.html#aws-ecr-for-private-docker-images","text":"AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials.","title":"AWS ECR for Private Docker Images"},{"location":"how-to-docs/elastic-container-registry.html#creating-a-docker-repository","text":"Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates.","title":"Creating a Docker Repository"},{"location":"how-to-docs/elastic-container-registry.html#generating-access-credentials","text":"Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it.","title":"Generating Access Credentials"},{"location":"how-to-docs/elastic-container-registry.html#accessing-a-docker-repository","text":"Accessing the AWS Container Registry to Pull & Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated.","title":"Accessing a Docker Repository"},{"location":"how-to-docs/elastic-container-registry.html#pre-requisites","text":"To follow the below steps you must have: * AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows * Docker (version 17.06 or above, check with docker --version )","title":"Pre-Requisites"},{"location":"how-to-docs/elastic-container-registry.html#step-1-retrieve-an-authorisation-token","text":"Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com","title":"Step 1: Retrieve an authorisation token"},{"location":"how-to-docs/elastic-container-registry.html#step-2-login-with-authorisation-token","text":"Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p <long-auth-token> https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Steps 1 and 2 will also not work if you are using AWS CLI (version 2.*). Instead use $ aws_account_id=\"340268328991\" $ aws_region=\"eu-west-2\" $ ecr_url=\"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" $ aws --region \"${aws_region}\" ecr get-login-password \\ | docker login \\ --password-stdin \\ --username AWS \\ \"${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\" Login Succeeded","title":"Step 2: Login with Authorisation Token"},{"location":"how-to-docs/elastic-container-registry.html#pulling-pushing-images","text":"Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f","title":"Pulling &amp; Pushing Images"},{"location":"how-to-docs/elastic-container-registry.html#listing-images-housekeeping","text":"Using the AWS CLI you can list all images that have been pushed to a given repository which you have access to. For example: $ aws ecr list-images --repository-name acp/hello-world-app { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" }, { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ] } To delete old images, you must have write access enabled or perform the action via a Robot Account. Images can be deleted based on a provided tag or digest. When providing a digest, all image tags with the same digest are deleted together. $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageTag=v0.0.1 { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"v0.0.1\" } ], \"failures\": [] } $ aws ecr batch-delete-image --repository-name acp/hello-world-app --image-ids imageDigest=sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f { \"imageIds\": [ { \"imageDigest\": \"sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f\", \"imageTag\": \"latest\" } ], \"failures\": [] }","title":"Listing Images &amp; Housekeeping"},{"location":"how-to-docs/elastic-container-registry.html#managing-image-deployments-via-drone-ci","text":"The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI.","title":"Managing Image Deployments via Drone CI"},{"location":"how-to-docs/ingress.html","text":"Using Ingress # An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: \"nginx-internal\" # Indicate the ingress SHOULD speak TLS between itself and pods (best-practice) ingress.kubernetes.io/backend-protocol: \"HTTPS\" name: myapp-server-internal spec: rules: - host: \"myapp.myproject.homeoffice.gov.uk\" http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - \"myapp.myproject.homeoffice.gov.uk\" # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Please view the official documentation for a full list of available ingress-nginx annotations . Note: Where the prefix for the annotation in the docs references nginx.ingress.kubernetes.io/ , this should be changed to ingress.kubernetes.io/ when running within ACP (as per the above example).","title":"Ingress"},{"location":"how-to-docs/ingress.html#using-ingress","text":"An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: \"nginx-internal\" # Indicate the ingress SHOULD speak TLS between itself and pods (best-practice) ingress.kubernetes.io/backend-protocol: \"HTTPS\" name: myapp-server-internal spec: rules: - host: \"myapp.myproject.homeoffice.gov.uk\" http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - \"myapp.myproject.homeoffice.gov.uk\" # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Please view the official documentation for a full list of available ingress-nginx annotations . Note: Where the prefix for the annotation in the docs references nginx.ingress.kubernetes.io/ , this should be changed to ingress.kubernetes.io/ when running within ACP (as per the above example).","title":"Using Ingress"},{"location":"how-to-docs/kubernetes-robot-token.html","text":"Getting a Kubernetes Robot Token # Users # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token. Project Admins (Creating a robot token) # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project -> Service -> Kube Robot Tokens).","title":"Kubernetes robot token"},{"location":"how-to-docs/kubernetes-robot-token.html#getting-a-kubernetes-robot-token","text":"","title":"Getting a Kubernetes Robot Token"},{"location":"how-to-docs/kubernetes-robot-token.html#users","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token.","title":"Users"},{"location":"how-to-docs/kubernetes-robot-token.html#project-admins-creating-a-robot-token","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project -> Service -> Kube Robot Tokens).","title":"Project Admins (Creating a robot token)"},{"location":"how-to-docs/kubernetes-user-token.html","text":"Getting a Kubernetes Token # Users # Log into the Platform Hub . Go to the Projects section and find your project. On the Overview & People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it. Project Admins (Creating a user token) # Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token.","title":"Kubernetes user token"},{"location":"how-to-docs/kubernetes-user-token.html#getting-a-kubernetes-token","text":"","title":"Getting a Kubernetes Token"},{"location":"how-to-docs/kubernetes-user-token.html#users","text":"Log into the Platform Hub . Go to the Projects section and find your project. On the Overview & People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it.","title":"Users"},{"location":"how-to-docs/kubernetes-user-token.html#project-admins-creating-a-user-token","text":"Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token.","title":"Project Admins (Creating a user token)"},{"location":"how-to-docs/network-policies.html","text":"Network Policies # By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: <your-namespace-here> spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on. Controlling Egress Traffic # Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples","title":"Network policies"},{"location":"how-to-docs/network-policies.html#network-policies","text":"By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: <your-namespace-here> spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on.","title":"Network Policies"},{"location":"how-to-docs/network-policies.html#controlling-egress-traffic","text":"Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: <your-namespace-here> spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples","title":"Controlling Egress Traffic"},{"location":"how-to-docs/performance-testing.html","text":"Run Performance Tests on a service hosted on ACP # As a Service, I should: # Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time Assessed tools summary: # An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana.","title":"Performance testing"},{"location":"how-to-docs/performance-testing.html#run-performance-tests-on-a-service-hosted-on-acp","text":"","title":"Run Performance Tests on a service hosted on ACP"},{"location":"how-to-docs/performance-testing.html#as-a-service-i-should","text":"Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time","title":"As a Service, I should:"},{"location":"how-to-docs/performance-testing.html#assessed-tools-summary","text":"An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana.","title":"Assessed tools summary:"},{"location":"how-to-docs/pod-security-policies.html","text":"Pod Security Policies # By default all user deployments will inherit a default PodSecurityPolicy applied accross our Kubernetes clusters, which define a set of conditions that a pod must be configured with in order to run successfully. runAsUser # This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user <username>, cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this: Dockerfile # Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer=\"rohith.jayawardene@digital.homeoffice.gov.uk\" RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5 Deployment Spec # In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: \"{{ .IMAGE_NAME }}\" image: \"{{ .IMAGE }}:{{ .VERSION }}\" ...","title":"Pod security policies"},{"location":"how-to-docs/pod-security-policies.html#pod-security-policies","text":"By default all user deployments will inherit a default PodSecurityPolicy applied accross our Kubernetes clusters, which define a set of conditions that a pod must be configured with in order to run successfully.","title":"Pod Security Policies"},{"location":"how-to-docs/pod-security-policies.html#runasuser","text":"This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user <username>, cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this:","title":"runAsUser"},{"location":"how-to-docs/pod-security-policies.html#dockerfile","text":"Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer=\"rohith.jayawardene@digital.homeoffice.gov.uk\" RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5","title":"Dockerfile"},{"location":"how-to-docs/pod-security-policies.html#deployment-spec","text":"In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: \"{{ .IMAGE_NAME }}\" image: \"{{ .IMAGE }}:{{ .VERSION }}\" ...","title":"Deployment Spec"},{"location":"how-to-docs/private-npm-registry.html","text":"Using artifactory as a private npm registry # A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN Setting up a local environment # Get your username and API key from artifactory Visit https://artifactory.digital.homeoffice.gov.uk/artifactory/webapp/#/profile, make a note of your username, and if you don't already have an API key then generate one. base64 encode your API key echo -n <api key> | base64 Set local environment variables Copy your encoded password, and set the following environment variables in your bash profile: export NPM_AUTH_USERNAME=<username> export NPM_AUTH_TOKEN=<base64 encoded api key> You might then need to source your profile to load these environment variables. Setting up CI in drone # Request a bot token for artifactory You can do this through the ACP Support Portal . One of the ACP team will create a token and send it to you as an encrypted gpg file via email. Decrypt the token gpg --decrypt ./path/to/file.gpg Add the token to drone as a secret First, base64 encode the token: echo -n \"<token>\" | base64 Then add this token to drone as a secret: drone secret add UKHomeOffice/<repo> NPM_AUTH_TOKEN <base64-encoded-token> --event pull_request Note: You will need to make sure the event types are lowercase. If an event is capitalised, it won't match the standard events inside of drone Note: you will need to make the secret available to pull request builds to be able to run npm commands in pull request steps Expose secret to build steps You will need to configure any steps which use npm to be able to access the secret. Do this by adding a secret property to those steps as follows: my_step: image: node:8 secrets: - npm_auth_token commands: - npm install - npm test Expose username to build steps In addition, you will need to add the username (as you provided when creating your token) as an environment variable. The easiest way to do this is as a \"matrix\" variable, which makes the username available to all steps without needing to configure them all individually. matrix: NPM_AUTH_USERNAME: - <username> Publishing modules to artifactory # It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name. Setting publish registry Add publishConfig to package.json. This ensures that the module can only ever be published to the private registry, and misconfiguration won't accidentally make it public \"publishConfig\": { \"registry\": \"https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/\" } Add auth settings In your project's .npmrc file (create one if it does not already exist) add the following lines: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:email=test@example.com The email address can be anything, it just needs to be set. Add publish step to drone Add the following step to your .drone.yml file to publish a new version whenever you release a tag. publish: image: node:8 secrets: - npm_auth_token commands: - npm publish when: event: tag Now, when you push new tags to github then drone should publish them to the artifactory npm registry automatically. Using modules from artifactory as dependencies # Configure your project to use artifactory In the project which is has private modules as dependencies, add the following line to .npmrc in the root of the project (create this file if it does not exist). @<namespace>:registry = https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ This will ensure that any module under that namespace will only ever install from artifactory, and never from the public registry If using multiple namespaces then add a line for each namespace. If the modules you are installing are not namespaced in artifactory, you can add the line with the namespace removed (i.e. registry = ... ) but this will have a negative impact on install speed. You should then add the following line to your project's .npmrc if they are not already there: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} You should now be able to install modules from artifactory into your local development environment. Installing dependencies in docker # If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} .","title":"Private npm registry"},{"location":"how-to-docs/private-npm-registry.html#using-artifactory-as-a-private-npm-registry","text":"A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN","title":"Using artifactory as a private npm registry"},{"location":"how-to-docs/private-npm-registry.html#setting-up-a-local-environment","text":"","title":"Setting up a local environment"},{"location":"how-to-docs/private-npm-registry.html#setting-up-ci-in-drone","text":"","title":"Setting up CI in drone"},{"location":"how-to-docs/private-npm-registry.html#publishing-modules-to-artifactory","text":"It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name.","title":"Publishing modules to artifactory"},{"location":"how-to-docs/private-npm-registry.html#using-modules-from-artifactory-as-dependencies","text":"","title":"Using modules from artifactory as dependencies"},{"location":"how-to-docs/private-npm-registry.html#installing-dependencies-in-docker","text":"If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} .","title":"Installing dependencies in docker"},{"location":"how-to-docs/pv-and-storage-classes.html","text":"Provisioned Volumes and Storage Classes # In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\" Backups for EBS # Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the Support Portal so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot .","title":"Pv and storage classes"},{"location":"how-to-docs/pv-and-storage-classes.html#provisioned-volumes-and-storage-classes","text":"In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\"","title":"Provisioned Volumes and Storage Classes"},{"location":"how-to-docs/pv-and-storage-classes.html#backups-for-ebs","text":"Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the Support Portal so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot .","title":"Backups for EBS"},{"location":"how-to-docs/ssl-passthrough.html","text":"TLS Passthrough # There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic) Example steps # First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: \"127.0.0.1\" - name: PROXY_SERVICE_PORT value: \"8080\" - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: \"FALSE\" - name: NAXSI_USE_DEFAULT_RULES value: \"FALSE\" - name: PORT_IN_HOST_HEADER value: \"FALSE\" volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" kubernetes.io/ingress.class: \"nginx-external\" name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: /","title":"Ssl passthrough"},{"location":"how-to-docs/ssl-passthrough.html#tls-passthrough","text":"There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic)","title":"TLS Passthrough"},{"location":"how-to-docs/ssl-passthrough.html#example-steps","text":"First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: \"127.0.0.1\" - name: PROXY_SERVICE_PORT value: \"8080\" - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: \"FALSE\" - name: NAXSI_USE_DEFAULT_RULES value: \"FALSE\" - name: PORT_IN_HOST_HEADER value: \"FALSE\" volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" kubernetes.io/ingress.class: \"nginx-external\" name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: /","title":"Example steps"},{"location":"how-to-docs/write-dockerfiles.html","text":"Writing Dockerfiles # Dockerfile best practice # We recommend familiarising yourself with Docker's excellent guidance on this topic. It is often easier to build from an existing base image. To find such base images that are maintained by Home Office colleagues, you can search the UKHomeOffice organisation on Github for repos starting with \u2018docker-\u2019 - e.g.: docker-java11-mvn If you want to use a base image in the UKHomeOffice organisation that does not appear to be regularly maintained, please get in touch via the ACP Service Desk and we will arrange write access to that repo. Please make sure that any base image that you maintain adheres to the best practices set out by Docker, and includes instructions to update all existing packages - e.g.: yum install -y curl && yum clean all && rpm --rebuilddb Home Office CentOS base image # If none of the technology specific images work for you, you can either build on top of them or build from the base Centos image .","title":"Write dockerfiles"},{"location":"how-to-docs/write-dockerfiles.html#writing-dockerfiles","text":"","title":"Writing Dockerfiles"},{"location":"how-to-docs/write-dockerfiles.html#dockerfile-best-practice","text":"We recommend familiarising yourself with Docker's excellent guidance on this topic. It is often easier to build from an existing base image. To find such base images that are maintained by Home Office colleagues, you can search the UKHomeOffice organisation on Github for repos starting with \u2018docker-\u2019 - e.g.: docker-java11-mvn If you want to use a base image in the UKHomeOffice organisation that does not appear to be regularly maintained, please get in touch via the ACP Service Desk and we will arrange write access to that repo. Please make sure that any base image that you maintain adheres to the best practices set out by Docker, and includes instructions to update all existing packages - e.g.: yum install -y curl && yum clean all && rpm --rebuilddb","title":"Dockerfile best practice"},{"location":"how-to-docs/write-dockerfiles.html#home-office-centos-base-image","text":"If none of the technology specific images work for you, you can either build on top of them or build from the base Centos image .","title":"Home Office CentOS base image"}]}